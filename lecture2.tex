% Copyright (C) 2014 by Massimo Lauria
% 
% Created   : "2014-01-07, Tuesday 17:01 (CET) Massimo Lauria"
% Time-stamp: ""
% Encoding  : UTF-8

% ---------------------------- USER DATA ------------------------------
\def\DataTitle{2. Semidefinite programming and Relaxation}
\def\DataTitleShort{Semidefinite programming}
\def\DataDate{28 January, 2014}
\def\DataDocname{Lecture 2 --- \DataDate}
\def\DataLecturer{Massimo Lauria}
\def\DataScribe{Massimo Lauria}
\def\DataKeywords{semidefinite programming, positive semidefinite matrices}

\def\DataAbstract{%
  Semidefinite programs have been proved to be valuable tools in
  approximation algorithms and in combinatorial optimization, since
  semidefinite relaxations are usually stronger than linear one. In
  lecture we describe what is a semidefinite program and we
  introduce hierarchies of sdp relaxations.}

% ---------------------------- PREAMBLE -------------------------------
\documentclass[a4paper,twoside,justified]{tufte-handout}
\usepackage{soscourse} % this is a non standard package
\begin{document} 
% --------------------------- DOCUMENT --------------------------------

\section{Positive semidefinite matrices}

A fundamental concept in semidefinite programming is the one of
\introduceterm{positive semidefinite matrices}. 
\begin{definition}
  A square matrix $A$ in $ \RR^{n\times n} $ is called positive
  semidefinite if 
  \begin{itemize}
    \item $A$ is symmetric\footnote{the condition of symmetry is often
      forgotten, but it is required.}, \ie, $ A=A^{T} $;
    \item for every $ x\in R^{n} $ it holds that $ x^{T} A x \geq 0 $.
  \end{itemize}
  A matrix $ A $ is \introduceterm{positive definite} if it is
  positive semidefinite and is also non-singular.
\end{definition}
We denote the set of positive semidefinite matrices in $\RR^{n\times
  n}$ as $\positivesemidefinite$.

\begin{fact}
  $ \positivesemidefinite $ is closed under positive
  combinations (\ie, is a cone): for a sequence $ M_{1}, M_{2}, \ldots, M_{\ell} $ of positive
  semidefinite matrices, the matrix 
\begin{equation}
  \alpha_{1} M_{1} + \alpha_{2} M_{2} + \cdots + \alpha_{\ell} M_{\ell}
\end{equation}
is positive semidefinite for $ \alpha_{1}\geq 0, \alpha_{2}\geq 0,
\ldots , \alpha_{\ell} \geq 0$.
\end{fact}

\section{Matrix decompositions}

Positive definite matrices have a lot of nice properties, and in
particular they have a set of useful decomposition. If a matrix $ A
\succ 0 $ then there are decompositions

\begin{align}
  A = U^{T}U & & \text{Cholesky decomposition}\\
  A = L D L^{T} & & \text{LDL decomposition}\\
  A = Q \Lambda Q^{T} & & \text{Spectral decomposition}
\end{align}

where $ U,L,D,Q,\Lambda $ are real matrices in $ \RR^{n\times n}
$. Furthermore $ U,L,D $ are unique.

In \introduceterm{spectral decomposition} (also called
\introduceterm{eigendecomposition}), matrix $ Q $ is an
\introduceterm{orthogonal matrix} which columns are unitary
eigenvectors of $ A $, \ie $ QQ^{T} = I$, and $ \Lambda $ is the
diagonal matrix containing the corresponding positive
eigenvectors\footnote{to prove this first you show that every
  eigenvalue is real and positive, and then that eingenspaces with
  different eigenvalues are orthogonal.}.
%
Consider the Euclidean geometry induced by the norm $ x^{T}x $ , then
the geometry induced by the norm $ x^{T}A x$ is an Euclidean one,
where the space is scaled along different axis.

In the \introduceterm{Cholesky decomposition} the matrix $ U $ is an
upper triangular real matrix where diagonal entries are positive. This
representation witnesses the fact that $ A $ is definite positive,
since $ x^{T}A x = x^{T}U^{T}U x = |Ux| > 0 $.

The \introduceterm{LDL decomposition} is a slight modification of
Cholesky decomposition. The matrix $ L$ is \introduceterm{lower unit
  triangular}, which is a lower triangular matrix with unit diagonal,
and $ D $ is a diagonal matrix. The decomposition exists for any
symmetric matrix, but for positive definite matrices the diagonal
matrix $D$ has positive entries. The peculiarity of this
decomposition is that the components of $ L $ and $ D $ are rational
functions of the entries of $ A $. Indeed the LDL decomposition is a
partial version of Cholesky decomposition which avoid computing square
roots. The decomposition for positive definite matrix $A$ follows by
induction from the equation (we have $\alpha>0$).
\begin{equation}\label{eq:matr-decomp}
  \begin{bmatrix}
    \alpha & v^{T} \\
    v & C
  \end{bmatrix} =
  \begin{bmatrix}
    1 & 0 \\
    v/\alpha & I
  \end{bmatrix} \cdot
  \begin{bmatrix}
    \alpha & 0 \\
    0 & C -vv^{T}/\alpha
  \end{bmatrix} \cdot
  \begin{bmatrix}
    1 & v^{T}/\alpha \\
    0 & I
  \end{bmatrix} 
\end{equation}

We need to show that $B = C - vv^{T}/\alpha $ is positive definite. Fix
non zero $ u\in\RR^{n-1} $ then fix $ x^{T}=[-u^{T}v/\alpha, u^{T}]
$. Then $ u^{T}Bu = x^{T}Ax > 0$.

Once you have LDL decomposition for positive definite matrices,
the Cholesky decomposition follows immediately.

All the decomposition can be extended to \stressterm{positive
  semidefinite} matrices, but then the entries of the diagonal of $ U
$ and $D$ can be zero; the matrices $ U $, $ L $, $ D $ are not uniquely
defined; there are some zero eigenvectors in $ \Lambda $. In
particular in equation~\eqref{sec:matr-decomp} we get that $ \alpha
\geq 0 $ by positive semidefiniteness, but if $ \alpha $ is zero, then
$ v $ must be the zero vector too, otherwise there would be $ y $ such that
$ y^{T}Ay \leq 0 $.

\begin{fact}
  Let $X$ and $A$ any symmetric matrices, then
\end{fact}

\section{Semidefinite programs}

Semidefinite programs can be naturally intepreted are a relaxation of
quadratic programs. Consider for example the $
\computationalproblem{MaxIndSet} $ on a graph $ G=([n],E) $, which is
expressed by the left quadratic program that follows.

\begin{figure*}
\begin{minipage}[t]{0.5\textwidth}
\begin{alignat*}{2}\label{eq:maxindset1}
  \maximize \sum_{i}x_{i}\\
  \subjectto x_{i}x_{j} =0 \qquad \text{\ if $\{i,j\}\in E$}\\
  & x^{2}_{i} -x_{i}= 0
\end{alignat*}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{alignat*}{2}
  \maximize \sum_{i}x^{2}_{i}\\
  \subjectto x_{i}x_{j} =0 \qquad \text{\ if $\{i,j\}\in E$}\\
  & x^{2}_{i} -x_{0}x_{i} =0\\
  & x^{2}_{0} = 1
\end{alignat*}
\end{minipage}
\end{figure*}


For convenience will consider a new dummy ``$x$'' variable $ x_{0} $,
and we homogenize the program, as shown before. 
%
The idea now is to relax the quadratic constraints by encoding the
products $x_{i}x_{j} $ as new variables $ y_{ij} $. Some of the
identities implied by the interpretation $ y_{ij} = x_{i}x_{j} $ are
enforced by the program, but not all of them can ---it would not be a
relaxation. We constrain $Y$ to be positive semidefinite.
%

\begin{alignat*}{2}
  \maximize \sum_{i}y_{ii}\\
  \subjectto y_{ij} =0 \qquad \text{\ if $\{i,j\}\in E$}\\
  & y_{ii} = y_{0i} \\
  & y_{00} = 1\\
  & Y \succeq 0
\end{alignat*}

A semidefinite program is a refinement of a linear program in which
the variables have a natural square matrix structure, and such
structure is a positive semidefinite matrix. We denote semidefinite
programs (\sdp) in one of this way (which are equivalent because of
the Cholesky decomposition).

\begin{figure}
\begin{minipage}[t]{0.5\textwidth}
\begin{alignat*}{2}
  \maximize \pointwiseprod{C}{X}\\
  \subjectto \pointwiseprod{A_{1}}{X} = b_{1}\\
  & \pointwiseprod{A_{2}}{X} = b_{2}\\
  & \vdots \\
  & \pointwiseprod{A_{\ell}}{X} = b_{\ell}\\
  & X \succeq 0
\end{alignat*}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{alignat*}{2}
  \maximize \sum_{i,j}c_{i,j} (v_{i}^{T}v_{j}) \\
  \subjectto \sum_{i,j} a_{i,j} (v^{T}_{i}v_{j})= b_{1}\\
  & \sum_{i,j} a^{2}_{i,j} (v^{T}_{i}v_{j})= b_{2}\\
  & \vdots \\
  & \sum_{i,j} a^{\ell}_{i,j} (v^{T}_{i}v_{j})= b_{\ell}\\
  & |v_{0}| = 1
\end{alignat*}
\end{minipage}
\end{figure}
%
Where $ A_i $ and $ C $ are symmetric matrices.  It is possible write
$\sdp$ in a more general fashion: we can have constraints on many
different positive semidefinite matrices, since if $ X_{1}\succeq 0 $
and $ X_{2} \succeq 0$ then $ \begin{bmatrix}
  X_{1} & 0 \\
  0 & X_{2}
\end{bmatrix} $ is positive semidefinite. Furthermore we can enforce
positive variables since $ \begin{bmatrix}
  x
\end{bmatrix} \succeq 0$ is equivalent to $ x \geq 0$. Furthermore we
can relax the condition of symmetry for matrices $ A_{i} $ and $ C $,
since $ X $ itself is required to be symmetric.

\section{Semidefinite programming duality (Cne programming duality)}

The duality of semidefinite programming is a more general (and
complex) version of duality of linear programs.
\begin{enumerate}
\item Define a cone, and convex cone.
\item Separation theorem for convex cones.
\item Farkas lemma for linear programming, and interpretation in cone
programming.
\item $ A(X) $ is not necessarily convex closed cone.
\item Farkas lemma with limit feasibility.
\item See lecture 12 of Gupta, O'Donnel.
\end{enumerate}


Consider a semidefinite program (let's ignore optimization now, focus
on decision). 
\begin{alignat*}{2}
  \pointwiseprod{A_{1}}{X} = b_{1}\\
  \pointwiseprod{A_{2}}{X} = b_{2}\\
  \vdots \\
  \pointwiseprod{A_{\ell}}{X} = b_{\ell}\\
  X \succeq 0
\end{alignat*}

We are going to witness unsatisfiability of a semidefinite program as
follow:



\section{Examples\ldots (probably not enough time)}

% ------------------------- EPILOGUE ------------------------------
\bibliography{soscourse}
\bibliographystyle{alpha}

\end{document} 


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
