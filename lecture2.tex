% Copyright (C) 2014 by Massimo Lauria
% 
% Created   : "2014-01-07, Tuesday 17:01 (CET) Massimo Lauria"
% Time-stamp: ""
% Encoding  : UTF-8

% ---------------------------- USER DATA ------------------------------
\def\DataTitle{2. Semidefinite programming and Relaxation}
\def\DataTitleShort{Semidefinite programming}
\def\DataDate{28 January, 2014}
\def\DataDocname{Lecture 2 --- \DataDate}
\def\DataLecturer{Massimo Lauria}
\def\DataScribe{Massimo Lauria}
\def\DataKeywords{semidefinite programming, positive semidefinite matrices}

\def\DataAbstract{%
  Semidefinite programs have been proved to be valuable tools in
  approximation algorithms and in combinatorial optimization, since
  semidefinite relaxations are usually stronger than linear one. In
  lecture we describe what is a semidefinite program and we
  introduce hierarchies of sdp relaxations.}

% ---------------------------- PREAMBLE -------------------------------
\documentclass[a4paper,twoside,justified]{tufte-handout}
\usepackage{soscourse} % this is a non standard package
\begin{document} 
% --------------------------- DOCUMENT --------------------------------

\section{Positive semidefinite matrices}

A fundamental concept in semidefinite programming is the one of
\introduceterm{positive semidefinite matrices}. 
\begin{definition}
  A square matrix $A$ in $ \RR^{n\times n} $ is called positive
  semidefinite if 
  \begin{itemize}
    \item $A$ is symmetric\footnote{the condition of symmetry is often
      forgotten, but it is required.}, \ie, $ A=A^{T} $;
    \item for every $ x\in R^{n} $ it holds that $ x^{T} A x \geq 0 $.
  \end{itemize}
  A matrix $ A $ is \introduceterm{positive definite} if it is
  positive semidefinite and is also non-singular.
\end{definition}
We denote the set of positive semidefinite matrices in $\RR^{n\times
  n}$ as $\positivesemidefinite$ and we say that $ A\succeq $ when $ A
\in \positivesemidefinite $.

\begin{fact}
  $ \positivesemidefinite $ is closed under positive
  combinations (\ie, is a cone): for a sequence $ M_{1}, M_{2}, \ldots, M_{\ell} $ of positive
  semidefinite matrices, the matrix 
\begin{equation}
  \alpha_{1} M_{1} + \alpha_{2} M_{2} + \cdots + \alpha_{\ell} M_{\ell}
\end{equation}
is positive semidefinite for $ \alpha_{1}\geq 0, \alpha_{2}\geq 0,
\ldots , \alpha_{\ell} \geq 0$.
\end{fact}

\section{Matrix decompositions}

Positive definite matrices have a lot of nice properties, and in
particular they have a set of useful decomposition. If a matrix $ A
\succ 0 $ then there are decompositions

\begin{align}
  A = U^{T}U & & \text{Cholesky decomposition}\\
  A = L D L^{T} & & \text{LDL decomposition}\\
  A = Q \Lambda Q^{T} & & \text{Spectral decomposition}
\end{align}

where $ U,L,D,Q,\Lambda $ are real matrices in $ \RR^{n\times n}
$. Furthermore $ U,L,D $ are unique.

In \introduceterm{spectral decomposition} (also called
\introduceterm{eigendecomposition}), matrix $ Q $ is an
\introduceterm{orthogonal matrix} which columns are unitary
eigenvectors of $ A $, \ie $ QQ^{T} = I$, and $ \Lambda $ is the
diagonal matrix containing the corresponding positive
eigenvectors\footnote{to prove this first you show that every
  eigenvalue is real and positive, and then that eingenspaces with
  different eigenvalues are orthogonal.}.
%
Consider the Euclidean geometry induced by the norm $ x^{T}x $ , then
the geometry induced by the norm $ x^{T}A x$ is an Euclidean one,
where the space is scaled along different axis.

In the \introduceterm{Cholesky decomposition} the matrix $ U $ is an
upper triangular real matrix where diagonal entries are positive. This
representation witnesses the fact that $ A $ is definite positive,
since $ x^{T}A x = x^{T}U^{T}U x = |Ux|^{2} > 0 $.

The \introduceterm{LDL decomposition} is a slight modification of
Cholesky decomposition. The matrix $ L$ is \introduceterm{lower unit
  triangular}, which is a lower triangular matrix with unit diagonal,
and $ D $ is a diagonal matrix. The decomposition exists for any
symmetric matrix, but for positive definite matrices the diagonal
matrix $D$ has positive entries. The peculiarity of this
decomposition is that the components of $ L $ and $ D $ are rational
functions of the entries of $ A $. Indeed the LDL decomposition is a
partial version of Cholesky decomposition which avoid computing square
roots. The decomposition for positive definite matrix $A$ follows by
induction from the equation (we have $\alpha>0$).
\begin{equation}\label{eq:matr-decomp}
  \begin{bmatrix}
    \alpha & v^{T} \\
    v & C
  \end{bmatrix} =
  \begin{bmatrix}
    1 & 0 \\
    v/\alpha & I
  \end{bmatrix} \cdot
  \begin{bmatrix}
    \alpha & 0 \\
    0 & C -vv^{T}/\alpha
  \end{bmatrix} \cdot
  \begin{bmatrix}
    1 & v^{T}/\alpha \\
    0 & I
  \end{bmatrix} 
\end{equation}

We need to show that $B = C - vv^{T}/\alpha $ is positive definite. Fix
non zero $ u\in\RR^{n-1} $ then fix $ x^{T}=[-u^{T}v/\alpha, u^{T}]
$. Then $ u^{T}Bu = x^{T}Ax > 0$.

Once you have LDL decomposition for positive definite matrices,
the Cholesky decomposition follows immediately.

All the decomposition can be extended to \stressterm{positive
  semidefinite} matrices, but then the entries of the diagonal of $ U
$ and $D$ can be zero; the matrices $ U $, $ L $, $ D $ are not uniquely
defined; there are some zero eigenvectors in $ \Lambda $. In
particular in equation~\eqref{eq:matr-decomp} we get that $ \alpha
\geq 0 $ by positive semidefiniteness, but if $ \alpha $ is zero, then
$ v $ must be the zero vector too, otherwise there would be $ y $ such that
$ y^{T}Ay \leq 0 $.


\section{Some useful facts about matrices}

\begin{fact}
  Let $ A=(a_{ij}) $, then $x^{T}A x = \pointwiseprod{(xx^{T})}{A}$
\end{fact}

\begin{fact}
  Let any two matrices $ A \in\RR^{n\times m}$ and $ B \in\RR^{m\times
    n} $. Then $ \trace{AB}= \trace{BA}$.
\end{fact}

\begin{fact}
  Let any two \stressterm{symmetric} $ A $ and $ B $. It holds that
  $ \trace{AB}=\pointwiseprod{A}{B} $.
\end{fact}

\begin{fact}\label{fact:positive_pointwise}
  For any symmetric $ n \times n $ matrix $ A $, $ A \in
  \positivesemidefinite $ if and only if $ \pointwiseprod{A}{B}\geq 0
  $ for all $ B \in \positivesemidefinite $.
\end{fact}
\begin{proof}
  If $ A $ and $ B $ as positive semidefinite, then by using the
  spectral decomposition we represent $ A $ as $Q^{T}\Lambda Q$.
  \begin{equation}
    \pointwiseprod{A}{B}=\trace{AB}=\trace{Q^{T}\Lambda Q B}
    =\trace{\Lambda Q B Q^{T}}. 
  \end{equation}
  Fix $ B' = Q B Q^{T} $, that is positive semidefinite, so all its
  diagonal entries are non negatives. Also $ \Lambda $ is a diagonal
  non negative matrix. Thus $
  \trace{AB}=\trace{\Lambda B'}=\sum_{i}\lambda_{i}b'_{ii} \geq 0$.
\end{proof}

\begin{fact}\label{fact:strict_positive_pointwise}
  For $ A \succ 0 $ it holds that $ \pointwiseprod{A}{B}>0 $ for all $
  B \in \positivesemidefinite $, unless $ B=0 $.
\end{fact}
The proof of the latter fact is identical to the proof of
Fact~\ref{fact:positive_pointwise}, with the remark that all
eigenvalues of a positive definite matrix are positive.


\section{Semidefinite programs}

Semidefinite programs can be naturally intepreted are a relaxation of
quadratic programs. Consider for example the $
\computationalproblem{MaxIndSet} $ on a graph $ G=([n],E) $, which is
expressed by the left quadratic program that follows.

\begin{figure*}
\begin{minipage}[t]{0.5\textwidth}
\begin{alignat}{2}
  \maximize \sum_{i}x_{i} \notag\\
  \subjectto x_{i}x_{j} =0 \qquad \text{\ if $\{i,j\}\in E$} \label{eq:maxindset1}\\
  & x^{2}_{i} -x_{i}= 0 \notag
\end{alignat}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{alignat}{2}
  \maximize \sum_{i}x^{2}_{i} \notag\\
  \subjectto x_{i}x_{j} =0 \qquad \text{\ if $\{i,j\}\in E$} \label{eq:maxindset2}\\
  & x^{2}_{i} -x_{0}x_{i} =0 \notag\\
  & x^{2}_{0} = 1 \notag
\end{alignat}
\end{minipage}
\end{figure*}


For convenience will consider a new dummy ``$x$'' variable $ x_{0} $,
and we homogenize the program, as shown before. 
%
The idea now is to relax the quadratic constraints by encoding the
products $x_{i}x_{j} $ as new variables $ y_{ij} $. Some of the
identities implied by the interpretation $ y_{ij} = x_{i}x_{j} $ are
enforced by the program, but not all of them can ---it would not be a
relaxation. We constrain $Y$ to be positive semidefinite.
%

\begin{alignat*}{2}
  \maximize \sum_{i}y_{ii}\\
  \subjectto y_{ij} =0 \qquad \text{\ if $\{i,j\}\in E$}\\
  & y_{ii} = y_{0i} \\
  & y_{00} = 1\\
  & Y \succeq 0
\end{alignat*}

A semidefinite program is a refinement of a linear program in which
the variables have a natural square matrix structure, and such
structure is a positive semidefinite matrix. Semidefinite programming
has found many uses in combinatorics and approximation
algorithms~\cite{gartner2012approximation}.
%
We denote semidefinite programs (\sdp) in one of this way (which are
equivalent because of the Cholesky decomposition).

\begin{figure}
\begin{minipage}[t]{0.5\textwidth}
\begin{alignat*}{2}
  \maximize \pointwiseprod{C}{X}\\
  \subjectto \pointwiseprod{A_{1}}{X} = b_{1}\\
  & \pointwiseprod{A_{2}}{X} = b_{2}\\
  & \vdots \\
  & \pointwiseprod{A_{\ell}}{X} = b_{\ell}\\
  & X \succeq 0
\end{alignat*}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{alignat*}{2}
  \maximize \sum_{i,j}c_{i,j} (v_{i}^{T}v_{j}) \\
  \subjectto \sum_{i,j} a_{i,j} (v^{T}_{i}v_{j})= b_{1}\\
  & \sum_{i,j} a^{2}_{i,j} (v^{T}_{i}v_{j})= b_{2}\\
  & \vdots \\
  & \sum_{i,j} a^{\ell}_{i,j} (v^{T}_{i}v_{j})= b_{\ell}\\
  & |v_{0}| = 1
\end{alignat*}
\end{minipage}
\end{figure}
%
Where $ A_i $ and $ C $ are symmetric matrices.  
%
A solution $X$ is called \introduceterm{strictly feasible} if $ X
\succ 0 $.
%
It is possible write
$\sdp$ in a more general fashion: we can have constraints on many
different positive semidefinite matrices, since if $ X_{1}\succeq 0 $
and $ X_{2} \succeq 0$ then $ \begin{bmatrix}
  X_{1} & 0 \\
  0 & X_{2}
\end{bmatrix} $ is positive semidefinite. Furthermore we can enforce
positive variables since $ \begin{bmatrix}
  x
\end{bmatrix} \succeq 0$ is equivalent to $ x \geq 0$. We can
translate inequality constraints into equality constraints using
positive slack variables. Furthermore we can relax the condition of
symmetry for matrices $ A_{i} $ and $ C $, since $ X $ itself is
required to be symmetric. Some forms of the semidefinite program are
more convenient than others, when it comes to prove theorem.

Another equivalent form of semidefinite program is the following:
\begin{alignat*}{2}
  \minimize b^{T}y\\
  \subjectto y_{1}A_{1} + y_{2}A_{2} + \cdots + y_{m}A_{m} \succeq C
\end{alignat*} 
where $ A_{i} $ and $ C $ are symmetric square matrices and $ y_{i} $
are scalar variables. This form is particularly handy when we come to
study duality.

\begin{exercise}
  Show that this latter form of semidefinite program is as expressive
  as the previous ones (\ie it is possible to express the same
  problems).
\end{exercise}

\section{Semidefinite programming duality}

The duality of semidefinite programming is a more general (and
complex) version of duality of linear programs. We will discuss its
proof here. Most of the proofs in this part comes from \Lovasz
lectures notes~\cite{lovasz2003semidefinite}.


First we describe a form of ``unsatisfiability'' proof for $ \sdp $
which is the key for semidefinite programming, as it was for linear
programming.

Consider a semidefinite program for an optimization problem. 
\begin{alignat}{2}
  \maximize \pointwiseprod{C}{X} \notag\\
  \subjectto \pointwiseprod{A_{1}}{X} = b_{1} \notag\\
  & \pointwiseprod{A_{2}}{X} = b_{2} \label{eq:sdp-primal}\tag{P}\\
  & \vdots \notag\\
  & \pointwiseprod{A_{m}}{X} = b_{m} \notag\\
  & X \succeq 0 \notag
\end{alignat}

Consider a combination of proof lines with coefficient $ y^{T}=
[y_{1}\ldots y_{m}]$, such that $ Y= \sum_{i}y_{i}A_{i} - C \succeq
0$. Then 
\begin{equation}
 \pointwiseprod{Y}{X}=\sum_{i}y_{i}b_{i} -\pointwiseprod{C}{X} 
\end{equation}
If $ X \succeq 0 $ then $\pointwiseprod{Y}{X} \geq 0$ by
Fact~\ref{fact:positive_pointwise} which means that $
\pointwiseprod{C}{X} \leq \sum_{i}y_{i}b_{i} $. This proves that there
is an upper bound to the maximization problem, and the smaller is the
upper bound the better.

\begin{definition}
  A semidefinite program in the form~\eqref{eq:sdp-primal} is called
  the \introduceterm{primal} program, and its \introduceterm{dual} is the program
  \begin{alignat}{2}
    \minimize  b^{T}y \notag \\
    \subjectto  y_{1}A_{1}+\cdots+y_{m}A_{m} -C \succeq
    0 \label{eq:sdp-dual} \tag{D}
  \end{alignat}
  A \introduceterm{strictly feasible} solution for this program is one
  such  $  y_{1}A_{1}+\cdots+y_{m}A_{m} -C \succ 0 $.
\end{definition}

The primal and the dual programs bound each other, assuming they have
feasible solution. We know that in linear programming the optimum
values for the two programs are the same. In semidefinite programming
the things are a little bit more tricky.  We start by proving a
version of Farkas Lemma for this framework.

\begin{lemma}[Farkas Lemma for semidefinite programs]
  \label{lmm:farkas_sdp}
  Let be $A_{1}, A_{2}, \ldots A_{m}$ and $B$ some $n \times n$
  symmetric matrices over the reals. Then the inequality
  \begin{equation}
    y_{1}A_{1}+y_{2}A_{2} + \cdots + y_{m}A_{m} - B \succ 0
  \end{equation}
  has no solutions $ y_{1}, y_{2}, \ldots,y_{m} $ if and only if there
  exists a symmetric matrix $ X\not=0 $
\begin{alignat*}{2}
  \pointwiseprod{A_{1}}{X} = 0\\
   \pointwiseprod{A_{2}}{X} = 0\\
   \vdots \\
   \pointwiseprod{A_{m}}{X} = 0 \\
   \pointwiseprod{B}{X} \geq 0\\
   X \succeq 0.
\end{alignat*}
\end{lemma}
\begin{proof}

  \stressterm{If part:} A for a solution $X$ for the second system
  implies that
  \begin{equation}
    \pointwiseprod{Y}{X}
    \leq 0
  \end{equation}
  for any $ Y=y_{1}A_{1}+y_{2}A_{2} + \cdots + y_{m}A_{m}-B $, and
  since $ X\succeq 0 $ and $ X\not=0 $ then by
  fact~\ref{fact:strict_positive_pointwise} it holds that $ Y $ is not
  positive definite\footnote{We will see that this part cannot be
    improved to provide proof of unsatisfiability in case the
    constraint is relaxed to $ Y\succeq 0 $.}.

  \stressterm{Only if:} This part is more difficult so we first prove
  it with $ B=0 $.

  Consider the linear subspace $ L\subseteq\RR^{n \times n} $
  generated by $y_{1}A_{1}+y_{2}A_{2} + \cdots + y_{m}A_{m}$.
  %
  If $ L $ is disjoint from the interior of the convex cone $
  \positivesemidefinite[n] $, then there is an hyperplane $ H $
  separating $ L $ and the interior of
  $\positivesemidefinite$\footnote{We use the hyperplane separator
    theorem, which proof is outside of the scope of this course.}.
  %
  Assume $H$ is characterized by some non trivial equation $
  \pointwiseprod{X}{Y}=0$ where $ X $ are the coefficients and $ Y $
  the variables. Matrix $ X $ can be made symmetric, and since the
  equation is non trivial, $ X\not=0 $.
  %
  $ L $ must be contained in $H$ otherwise $ L $ would not be in one
  of the halfspaces. It follows that $ \pointwiseprod{X}{A_{i}}=
  \pointwiseprod{A_{i}}{X}=0 $ for every $ i $. 
  %
  For every $ Y\in\positivesemidefinite[n] $ the separator $ X $ has $
  \pointwiseprod{X}{Y}\geq 0 $ which implies $ X \succeq 0 $
  because of fact~\ref{fact:positive_pointwise}.  
 
  Now we go back to $ B\not=0$. We consider the new constraint
  \begin{equation}
    y_{1}\begin{bmatrix}A_{1} & 0\\ 0 & 0\end{bmatrix}
    +y_{2}\begin{bmatrix}A_{2} & 0\\ 0 & 0\end{bmatrix} + \cdots
    +y_{m}\begin{bmatrix}A_{m} & 0\\ 0 & 0\end{bmatrix} 
    + z \begin{bmatrix} -B & 0\\ 0 & 1\end{bmatrix} \succ 0
  \end{equation}
  which is equivalent to the former one: if $ z=0 $ then the sum
  is not positive definite, so $ z\not=0 $ in any solution. 

  If there is no such solution there must be a non zero $ n+1 \times
  n+1 $ matrix $ X'\succeq 0 $. Let us call $X$ the submatrix made by
  its $ n $ first columns and rows. Then $ \pointwiseprod{A_{i}}{X}
  =0$ and $ \pointwiseprod{\begin{bmatrix} -B & 0\\ 0 &
      1\end{bmatrix}}{X'} = -\pointwiseprod{B}{X}+x'_{n+1,n+1} = 0
  $. By semidefiniteness of $ X'$ then $ x'_{n+1,n+1}\geq 0$ and thus
  $ \pointwiseprod{B}{X}\geq 0 $.
\end{proof}

We are now set to prove the duality theorem. In a way similar to
linear programming duality, the Farkas lemma provides a general way to
witness unsatisfiability of the primal by the means of the dual, and
vice versa. Unfortunately for the case of semidefinite programming
there is some condition in order to get strong duality\footnote{This
  is called \introduceterm{Slater condition} and apply in general to
  convex programming. If both the primal and the dual have strictly
  feasible solutions, \ie solutions which are in the interior of the
  convex body, then strong duality in convex programming
  follows. Notice that the solutions do not need to be optimal
  ones.}.


\begin{theorem}\label{thm:strongduality}
  Assume both primal and dual have feasible solutions. Then $v_{P}
  \geq v_{D}$, where $v_{P}$ and $ v_{D} $ are the optimal values to
  the primal and dual respectively. Moreover, if the dual has a
  strictly feasible solution then
  \begin{enumerate}
    \item The primal optimum is attained
    \item $ v_{P} = v_{D}$ 
  \end{enumerate}
  Similarly, if the dual is strictly feasible, then the primal optimal
  value is achieved, and equals the dual optimal value. Hence, if both
  the primal and dual have strictly feasible solutions, then both
  $v_{P}$ and $v_{D}$ are attained.
\end{theorem}

\begin{proof}
  Let be $X$ any solution for program~\eqref{eq:sdp-primal} and $ y $
  a solution for program~\eqref{eq:sdp-dual}. By
  Fact~\ref{fact:positive_pointwise} we get that
  \begin{equation}
    0 \leq \pointwiseprod{\sum_{i} y_{i} A_{i}-C}{X} = y^{T}b - \pointwiseprod{C}{X} 
  \end{equation}
  which means that $ v_{P} \leq v_{D}$\footnote{This part of the
    theorem is often called weak duality theorem.}.

  Furthermore the constraints $ b^{T}y < v_{D} $ and $
  \sum_{i}y_{i}A_{i} \succeq C $ have no feasible solution by
  definition of the dual. Thus consider the matrices 
  \begin{equation}
    A'_{i} = \begin{bmatrix}
      -b_{i} & 0\\
      0 & A_{i}
    \end{bmatrix}
    \qquad
    C' = \begin{bmatrix}
      -v_{D} & 0\\
      0 & C
  \end{bmatrix}
  \end{equation}
  then the system $ \sum_{i}y_{i}A'_{i} \succ C' $ is unsatisfiable
  and then by Farkas lemma there is a non zero positive semidefinite
  matrix $X'$ such that $ \pointwiseprod{A'_{i}}{X'} = 0 $ for all $
  1\leq i\leq m $ and $\pointwiseprod{C'}{X'} \geq 0 $. Let us write
  \begin{equation}
    X'=\begin{bmatrix}
      x_{00} & x^{T}\\
      x & X
    \end{bmatrix}
  \end{equation}
  so we get that 
  \begin{equation}\label{eq:scaled_primal_solution}
    \pointwiseprod{A_{i}}{X} = x_{00}b_{i} \quad 
    \text{for $i=1\ldots m$}
    \qquad 
    \pointwiseprod{C}{X} \geq x_{00}v_{d}.
  \end{equation}
  We claim that $ x_{00} \not= 0$ since otherwise $ X $ solution would
  be a witness (by Farkas Lemma) of the unsolvability of
  program~\eqref{eq:sdp-dual} by strict feasible solutions, and this
  contradicts our assumptions. Since $ x_{00}\not=0 $ we can assume $
  x_{00}=1 $ by scaling the solution in~\eqref{eq:scaled_primal_solution}. 

  After rescaling, $ X $ is feasible a solution of
  program~\eqref{eq:sdp-primal} with value $ \pointwiseprod{C}{X}\geq
  v_{D} $. Thus we get that $ v_{D}\leq v_{P} $.
\end{proof}

\begin{exercise}
  In the proof of Theorem~\ref{thm:strongduality} we showed only that
  if the dual has strictly feasible solution then the primal optimum
  is attained. Please prove that if the primal has strictly feasible
  solution then the dual optimum is attained as well.
\end{exercise}

\section{Examples of \sdp\ with bad behavior}

In the proof of the strong duality theorem
(Theorem~\ref{thm:strongduality}) the optimum is attained by assuming
that the there is a strictly feasible dual solution. Furthermore we
used that to prove that primal and dual optimum are the same.  We will
see classic examples from\cite{lovasz2003semidefinite} in which the
two optimum are not the same (\ie\ there is a \introduceterm{duality
  gap}) or in which the optimum is only reached in the limit.

\begin{example}\label{eg:sdp-limit-solution}
  Consider the program
  \begin{alignat*}{2}
    \maximize -x_{1}\\
    \subjectto \begin{bmatrix}
      x_{1} & 1 & 0\\
      1 & x_{2} & 0 \\
      0 & 0 & x_{1}
    \end{bmatrix}\succeq 0
  \end{alignat*}

  Because of the positive semidefinite requirement, we get that $
  x_{1},x_{2}\geq 0 $ and that $ x_{1}x_{2}\geq 1 $. The objective
  function  $ -x_{1} $ can get arbitrarily close to $ 0 $ but will
  stay below it. This $ v_{P}=0 $ and there is no solution which
  actually get to that value. 
\end{example}

So in the previous program we have an optimum, which is reached only
in the limit.

\begin{example}\label{eg:sdp-duality-gap}
  Consider the primal program 
  \begin{alignat}{2}
    \maximize -x_{33}\\
    \subjectto x_{12} + x_{21} + x_{33} = 1\\
    & x_{22}=0\\
    & X \succeq 0
  \end{alignat}
  The feasible solutions for this programs are matrices of the form 
  \begin{equation}
    \begin{bmatrix}
      a & 0 & b\\
      0 & 0 & 0\\
      b & 0 & 1
    \end{bmatrix}
  \end{equation}
 which $ a \geq b^{2} $ and $ v_{P}=-1 $. Now we consider its dual
 program
  \begin{alignat}{2}
    \minimize y_{1}\\
    \subjectto y_{1} \begin{bmatrix}
      0 & 1 & 0\\
      1 & 0 & 0\\
      0 & 0 & 1
    \end{bmatrix} + 
    y_{2} \begin{bmatrix}
      0 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 0
    \end{bmatrix} 
    \succeq 
    \begin{bmatrix}
      0 & 0 & 0\\
      0 & 0 & 0\\
      0 & 0 & -1
    \end{bmatrix}
  \end{alignat}
  Dual solutions have $ y_{1}=0 $ and $ y_{2}\geq 0 $, thus $ v_{D}=0
  $. See that $ v_{P} < v_{D} $, thus there is a duality gap of $ -1 $.
\end{example}

\begin{example}\label{eg:sdp-close-gap}
  Consider a variant of the program in
  Example~\ref{eg:sdp-duality-gap} where $ x_{11} $ is involved in the
  optimized function. This example comes from\cite{gupta2011lecture12}.
  \begin{alignat}{2}
    \maximize -\epsilon x_{11} -x_{33}\\
    \subjectto x_{12} + x_{21} + x_{33} = 1\\
    & x_{22}=0\\
    & X \succeq 0
  \end{alignat}
  The feasible solutions for this programs do not change, and are
  still matrices of the form
  \begin{equation}
    \begin{bmatrix}
      a & 0 & b\\
      0 & 0 & 0\\
      b & 0 & 1
    \end{bmatrix}
  \end{equation}
  where $ a \geq b^{2} $ and so $ v_{P}=-1$ as in the previous case,
  since it pays to make $ a$ as small as possible and we can fix it to
  be $ 0 $. Now we consider its dual program
  \begin{alignat}{2}
    \minimize y_{1}\\
    \subjectto y_{1} \begin{bmatrix}
      0 & 1 & 0\\
      1 & 0 & 0\\
      0 & 0 & 1
    \end{bmatrix} + 
    y_{2} \begin{bmatrix}
      0 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 0
    \end{bmatrix} 
    \succeq 
    \begin{bmatrix}
      -\epsilon & 0 & 0\\
      0 & 0 & 0\\
      0 & 0 & -1
    \end{bmatrix}\label{eq:dual_constraint}
  \end{alignat}
  Now see that with $ y_{1}=-1 $ and $ y_{2}= \frac{1}{\epsilon} $ the
  constrain matrix in \eqref{eq:dual_constraint} is
  \begin{equation}
    \begin{bmatrix}
      \epsilon & -1 & 0\\
      -1 & \frac{1}{\epsilon} & 0\\
      0 & 0 & 0
    \end{bmatrix} = \begin{bmatrix}
      \sqrt{\epsilon} \\
      -\frac{1}{\sqrt{\epsilon}}\\
      0
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
      \sqrt{\epsilon} & -\frac{1}{\sqrt{\epsilon}} & 0.
    \end{bmatrix}
  \end{equation}
  Since the matrix is positive semidefinite then the dual solution is
  feasible and $ v_{D}=-1 $, equal to the primal optimum.
\end{example}



% ------------------------- EPILOGUE ------------------------------
\bibliography{soscourse}
\bibliographystyle{alpha}

\end{document} 


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
