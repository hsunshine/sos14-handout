% Copyright (C) 2014 by Massimo Lauria
% 
% Created   : "2014-01-07, Tuesday 17:01 (CET) Massimo Lauria"
% Time-stamp: "2014-01-14, 23:35 (CET) Massimo Lauria"
% Encoding  : UTF-8

% ---------------------------- USER DATA ------------------------------
\def\DataTitle{1. Linear relaxations of integer programs.}
\def\DataTitleShort{Linear programming}
\def\DataDate{27 January, 2014}
\def\DataDocname{Lecture 1 --- \DataDate}
\def\DataLecturer{Massimo Lauria}
\def\DataScribe{Massimo Lauria}
\def\DataKeywords{integer programming, linear programming,
  \Lovasz-\Schrijver, Sherali-Adams, }

\def\DataAbstract{%
  We can express combinatorial problems using integer programs, but
  since we can't solve them, we consider relaxed linear programs and
  we round fractional solutions to integer. We consider hierarchies of
  linear programs and discuss the quality of the corresponding
  solutions.}


% ---------------------------- PREAMBLE -------------------------------
\documentclass[a4paper,twoside,justified]{tufte-handout}
\usepackage{soscourse} % this is a non standard package
\begin{document} 
% --------------------------- DOCUMENT --------------------------------

This lecture is a sort of scaled down demo of the rest of the
course. Here we are see that we can express decisions and optimization
problems by the means of \introduceterm{integer programs}. This translation in
even possible for $ \NP $-hard problems, thus there is not efficient
algorithm to solve integer programs unless $\PTIME=\NP$, which is
considered by many to be very unlikely\footnote{Every hardness result
  we will see during the course won't rely on any unproved assumption.}.

In any case there are no \stressterm{known} efficient algorithm to
solve integer programs, and a viable strategy is to
\introduceterm{relax} the integer program to something more
manageable: for example a liner program.

The most naive way to do that is to transform the integrality
constraints into fractional linear constraints, \eg $ x \in\{0,1\} $
into $ 0 \leq x \leq 1 $, and leave the other constraints as they
are\footnote{We can assume that all such constraints are affine,
  namely of the three forms
  \begin{equation*}
    \sum_{i}a_{i}x_{i} \leq b \quad   \sum_{i}a_{i}x_{i} \geq b \quad \sum_{i}a_{i}x_{i} = b
  \end{equation*}
  for $a_i$ and $ b $ in \RR}.

Once we relax the integer program we have a lot of new fractional
solution that are not allowed in the integer one. For example consider
the program that characterizes the maximum independent sets of graph $
K_{3} $, \ie, the triangle.

\begin{alignat}{2}
  \maximize x_{1} + x_{2} + x_{3} \notag\\
  \subjectto   x_{1} + x_{2} \leq 1 \notag\\
             & x_{2} + x_{3} \leq 1 \label{eq:triangle-ind-set}\\
             & x_{1} + x_{3} \leq 1 \notag\\
             & \varboolean{x_{1}},\ \varboolean{x_{2}},\ \varboolean{x_{3}}.\notag
\end{alignat}

Its \introduceterm{integer optimum} is 1 since no two variables can be
both 1, but if we relax the constraints and allow $ \varbounded{x_{i}}
$, then the linear program has a \introduceterm{fractional optimum}
of $ \frac{3}{2} $, by setting all variables to $\frac{1}{2}$.

Most of the course will study systematic techniques to improve the
relaxation, adding variables and inequalities in order to
\stressterm{cut away} feasible fractional solutions without changing
the set of integer solutions. The quality and the complexity of such
techniques is controlled by a parameter called \introduceterm{rank}:
larger the rank, less fractional solution remain.

\section{Linear programming}

The most classic and studied optimization formalism is
\introduceterm{linear programming}, which is illustrated by books as
\cite{matousek2007understanding}: we want to optimize (either minimize
of maximize) a linear function $ \sum_{i} c_{i}x_{i} $ over the set of
variables $ \{x_{i}\}^{n}_{i=1} $, which are constrained to satisfy a
set of linear inequalities and linear equations. Without loss of
generality we can assume a linear program to have one of the following
forms\footnote{While we will discuss the left form more often, the
  right one is also very common and is called the
  \introduceterm{standard form}. The standard form is particularly
  useful in the simplex implementation.}.

\begin{figure}
\begin{minipage}[t]{0.5\textwidth}
\begin{alignat*}{2}
  \maximize c^{T}x\\
  \subjectto Ax \leq b
\end{alignat*}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{alignat}{2}\label{eq:lpforms}
  \maximize c^{T}x \notag\\
  \subjectto Ax = b\\
  & x \geq 0\notag
\end{alignat}
\end{minipage}
\end{figure}


In order to optimize a linear program it is of course necessary to
understand is the program is \stressterm{feasible}, \ie if the is a
value of $ x $ that satisfies, for example the linear inequalities $Ax
\leq b$. Thus linear programs can describe both \stressterm{decision} and
\stressterm{optimization} problems.

The striking feature of linear program is the possibility to
\stressterm{witness} the unsatisfiability of a set of linear
inequalities. This feature is formalized by \introduceterm{Farkas
  Lemma}: as it happens often with fundamental results, there are
several ways to state Farkas Lemma.

\begin{lemma}[Farkas Lemma]
  A set of linear inequalities $ Ax \leq b $ is unsatisfiable if and
  only if there exists a positive vector $ y \geq 0$ such that $
  y^{T}A = 0$ and $y^{T}b = -1$.
\end{lemma}

\begin{marginfigure}
  % \introduceterm{Proof complexity} interpretation. Later we will
  % discuss a little the proof complexity of unsatisfiability:  namely
  % how large and how hard to find is a proof that a set of constraints
  % is unsatisfiable of that a function cannot be optimized pass a
  % certain threshold.
  Given two inequalities $ a^{T}x \leq b $ and $c^{T}x \leq d$ then
  any positive combination is a semantic consequence. Thus we can
  design a proof system $\proofsystem{LP}$ for sets of linear
  inequalities with the following inference rule,
  \begin{equation*}
    \frac{a^{T}x \leq b \quad c^{T}x \leq d}{(\alpha a + \gamma c)x
      \leq (\alpha b + \gamma d)x  }
  \end{equation*}
  with $\alpha \geq 0, \gamma \geq 0$. Farkas Lemma states that such
  proof system can always deduce a contradiction $ 0 \leq -1 $ from a
  set of unsatisfiable linear inequalities.
\end{marginfigure}

If the linear program is used for optimization, it is interesting that
we can take positive combinations to prove bounds on the function to
be optimized. Start with the \introduceterm{primal} program that asks to minimize $c^{T}x$
under constraints $ Ax \leq b, x\geq 0$ (notice that we highlighted
the non-negativity constraints). Now consider a non negative vector $
y^{T}$ such that $ y^{T}A \geq c^{T}$ and such that $ y^{T}b = v$. For
every feasible solution $ x $ of the primal program it holds that 
\begin{equation}
  c^{T} x \leq y^{T}Ax \leq y^{T} b\tag{weak duality}\label{eq:weak_duality}
\end{equation}
Thus the solution $ y\geq 0 $ witnesses the \stressterm{upper bound} $
b^{T}y $ to the maximum achievable in the primal
program. \stressterm{How tight is such upper bound?} We can answer
that by looking at \introduceterm{dual}
program~\eqref{eq:dualprogram}. Notice also that the dual
of~\eqref{eq:dualprogram} is~\eqref{eq:primalprogram}.


\begin{figure}
\begin{minipage}[t]{0.5\textwidth}%
\begin{alignat}{2}
  \maximize c^{T}x\notag\\
\label{eq:primalprogram}\subjectto Ax \leq b \tag{P}\\
  x\geq 0 \notag
\end{alignat}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}%
\begin{alignat}{2}
  \minimize b^{T}y \notag\\
  \label{eq:dualprogram}  \subjectto A^{T}y \leq c \tag{D}\\
  & y \geq 0\notag
\end{alignat}
\end{minipage}
\end{figure}

We saw that Farkas Lemma gives a complete solution of the decision
case. From the same Farkas Lemma it is possible to prove the
\introduceterm{duality theorem}, which basically claims that there are
solutions of the dual program that witness tight bounds on the primal
program (and viceversa).

\begin{marginfigure}
  As in the case of Farkas Lemma,
  Theorem claims that proof system $\proofsystem{LP}$ is complete in
  the sense that can prove all valid linear inequalities.
\end{marginfigure}

\begin{theorem}[Duality theorem]
  Consider the linear programs~\eqref{eq:primalprogram}
  and~\eqref{eq:dualprogram}, exactly one of the following happens
  \begin{itemize}
    \item Neither~\eqref{eq:primalprogram} nor~\eqref{eq:dualprogram} have a
    feasible solution.
    \item Program~\eqref{eq:primalprogram} has solutions with
    arbitrarily large value, and program~\eqref{eq:dualprogram} is
    unfeasible.
    \item Program~\eqref{eq:dualprogram} has solutions with
    arbitrarily small value, and program~\eqref{eq:primalprogram} is
    unfeasible.
    \item Both~\eqref{eq:primalprogram} and~\eqref{eq:dualprogram}
    have optimal solutions. Let $ x^{*}$ and $ y^{*}$ such solutions, then
    \begin{equation*}
      c^{T} x^{*}=b^{T}y^{*}.
    \end{equation*}
  \end{itemize}
\end{theorem}

\section{Complexity of Linear programming}

Deciding the satisfiability of a set of linear inequalities is clearly
in \NP\@.  Farkas Lemma and Duality Theorem shows that also deciding
its unsatisfiability is in \NP, thus the problem is in $ \NP \cap
\coNP $. But actually there are well known efficient algorithms for
this problem. All of them are based the geometric interpretation of
the system of linear inequalities as a convex polyhedron. 
\begin{itemize}
  \item The \introduceterm{simplex method} is the first algorithm
  invented for linear programming and has been invented by (Dantzig,
  1947). It does not run in polynomial time, but it is quite fast in
  practice. The idea is to walk on the edges of the polyhedron induced
  by the linear program, in order to reach the vertex with optimal
  value.
  \item The first polynomial time algorithm for linear programming is
  based on the \introduceterm{ellipsoid method} (Khachyian, 1979). The
  simplex is much faster in practice, but of course the ellipsoid has
  great theoretical value, and runs under more general conditions. For
  example super polynomial size linear programs, assuming the program
  has an efficient \introduceterm{separator}\footnote{A separator is
    an oracle that, given a point $x$ outside of the set of feasible
    solutions $F$, outputs an hyperplan separating $ p$ from $ F$.}.
  The algorithms assumes that the set of feasible solutions is in a
  0-centered ball of large enough radius, and completely
  contains some ball of range $ \epsilon $.
  %
  The algorithm check whether the center of the ball is feasible. If
  it is not then the separator oracle suggest which half of the ball
  to save and which to forget. A new ellipsoid of smaller volume
  ``encircles'' the useful half.  The idea is to encircle the set of
  feasible solutions with smaller and smaller ellipsoids, until either
  the center of the one ellipsoid is feasible or the ellipsoid is so
  small that cannot contain a ball of range $ \epsilon $.
  
  \item Since the ellipsoid method is so inefficient in practice, the
  simplex is usually preferred. The advent of \introduceterm{interior
    point methods} changed the scenario, providing a family of
  polynomial time algorithms which are fast in practice. The first
  member of such family is due to Karmakar (1984): his algorithm is
  based on the idea of exploring the space of solution walking through
  the interior of the polytope. The original algorithm was not faster
  than the simplex method but nowadays there are competitive
  improvements and variants. We must observe that interior point
  methods were already used in the context of non-linear programming
  way before than Karmakar algorithm.
\end{itemize}

\begin{marginfigure}
\begin{example}
  Consider the problem of finding a perfect matching in the graph,
  which is expressed by the following program.
  \begin{alignat*}{2}
    e_{11} + e_{12} & \leq 1\\
    e_{22} + e_{}
  \end{alignat*}
\end{example}
\end{marginfigure}

\section{Integer programs and Linear relaxations}

Most problems in computer science require solutions that are discrete,
integer or $ \{0,1\} $. It is not possible express such constraints as
linear inequalities, so we need a new formalism called
\introduceterm{integer program}. An integer program may have the
following form.

\begin{alignat*}{2}
  \maximize  c^{T}x\\
  \subjectto Ax \leq b\\
  & x_{i} \in\{0,1\} \text{\ or\ } x_{i} \in \ZZ
\end{alignat*}

It is pretty clear that integer programs are in $ \NP $, and that they
are expressive enough to encode $\NP$-complete problems, thus Integer
programming is $\NP$-complete.
%
With a \introduceterm{linear relaxation} we trade the expressibility
of the program with and efficient solver. We drop all non-linear
constraints and we substitute them with fractional ones (\eg, $
x_{i}\in \ZZ $ becomes $ x_{i}\in\RR $ and $ x_{i} \in\{0,1\} $
becomes $ 0\leq x_{i}\leq 1 $).



\section{Integrality gaps}


\section{Improving the linear relaxations}


\section{\Lovasz-\Schrijver\ hierarchy}

\subsection{Rank lower bounds}

\section{Sherali-Adams hierarchy}

\subsection{Rank lower bounds}


% ------------------------- EPILOGUE ------------------------------
\bibliography{theoryofcomputing,soscourse}
\bibliographystyle{alpha}

\end{document} 


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
