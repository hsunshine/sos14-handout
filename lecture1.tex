% Copyright (C) 2014 by Massimo Lauria
% 
% Created   : "2014-01-07, Tuesday 17:01 (CET) Massimo Lauria"
% Time-stamp: "2014-01-27, 23:16 (CET) Massimo Lauria"
% Encoding  : UTF-8

% ---------------------------- USER DATA ------------------------------
\def\DataTitle{1. Linear relaxations of integer programs.}
\def\DataTitleShort{Linear programming}
\def\DataDate{27 January, 2014}
\def\DataDocname{Lecture 1 --- \DataDate}
\def\DataLecturer{Massimo Lauria}
\def\DataScribe{Massimo Lauria}
\def\DataKeywords{integer programming, linear programming,
  \Lovasz-\Schrijver, Sherali-Adams}

\def\DataAbstract{%
  We can express combinatorial problems using integer programs and,
  since we can't solve them, we consider relaxed programs in which
  integer constraints are relaxed to linear ones. Then we round
  fractional solutions to integer. We consider hierarchies of linear
  programs and discuss the quality of the corresponding solutions.}


% ---------------------------- PREAMBLE -------------------------------
\documentclass[a4paper,twoside,justified]{tufte-handout}
\usepackage{soscourse} % this is a non standard package
\begin{document} 
% --------------------------- DOCUMENT --------------------------------

This lecture is a sort of scaled down demo of the rest of the
course. Here we see that we can express decisions and optimization
problems by the means of \introduceterm{integer programs}. This
translation works for $ \NP $-hard problems, thus there cannot be
efficient algorithms to solve integer programs unless $\PTIME=\NP$,
which is considered by many to be very unlikely\footnote{Most of the
  hardness results we will see during the course won't rely on any
  unproved assumption.}.

In any case there is no \stressterm{known} efficient algorithm that
solves integer programs; a viable strategy is to \introduceterm{relax}
the integer programs to something easier to manage: for example liner
programs.

The most naive way to do that is to transform the integral constraints
into fractional linear constraints, \eg $ x \in\{0,1\} $ into $ 0 \leq
x \leq 1 $, and leave the other constraints alone\footnote{We can
  assume that all such constraints are affine, namely they have one of
  the three forms
  \begin{equation*}
    \sum_{i}a_{i}x_{i} \leq b \quad   \sum_{i}a_{i}x_{i} \geq b \quad \sum_{i}a_{i}x_{i} = b
  \end{equation*}
  for $a_i$ and $ b $ in \RR}.

Once we relax the integer program we have a lot of new fractional
solutions that were not allowed before. Consider, for example,
the program that characterizes the maximum independent sets of graph $
K_{3} $, \ie, the triangle.

\begin{alignat}{2}
  \maximize x_{1} + x_{2} + x_{3} \notag\\
  \subjectto   x_{1} + x_{2} \leq 1 \notag\\
             & x_{2} + x_{3} \leq 1 \label{eq:triangle-ind-set}\\
             & x_{1} + x_{3} \leq 1 \notag\\
             & \varboolean{x_{1}},\ \varboolean{x_{2}},\ \varboolean{x_{3}}.\notag
\end{alignat}

Its \introduceterm{integer optimum} is 1, but if we relax the
constraints and allow $ \varbounded{x_{i}} $, then the linear program
has a \introduceterm{fractional optimum} of $ \frac{3}{2} $, by
setting all variables to $\frac{1}{2}$.

During most of the course we will study systematic techniques to
improve the relaxation: we add variables and inequalities in order to
\stressterm{cut away} feasible fractional solutions without changing
the set of integer solutions. The quality and the complexity of such
techniques is controlled by a parameter called \introduceterm{rank}:
the larger the rank, the fewer fractional solutions remain.

\section{Linear programming}

The most studied optimization formalism is \introduceterm{linear
  programming}, which is illustrated by books as
\cite{matousek2007understanding}: we want to optimize (either minimize
of maximize) a linear function $ \sum_{i} c_{i}x_{i} $ over the set of
variables $ \{x_{i}\}^{n}_{i=1} $, which are constrained to satisfy a
set of linear inequalities and linear equations. Without loss of
generality we can assume a linear program to have one of the following
forms\footnote{While we will discuss the left form more often. The
  right one is also very common and is called the
  \introduceterm{standard form}. The standard form is particularly
  useful in implementations of the simplex algorithm.}.

\begin{figure}
\begin{minipage}[t]{0.5\textwidth}
\begin{alignat*}{2}
  \maximize c^{T}x\\
  \subjectto Ax \leq b
\end{alignat*}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{alignat}{2}\label{eq:lpforms}
  \maximize c^{T}x \notag\\
  \subjectto Ax = b\\
  & x \geq 0\notag
\end{alignat}
\end{minipage}
\end{figure}


In order to optimize a linear program it we need to check if the
program is \stressterm{feasible}, \ie if there is a value of $ x $
that satisfies the linear constraints. Indeed we can use linear
programs to describe either \stressterm{decision} or
\stressterm{optimization} problems.

The striking feature of linear programming is the possibility to
\stressterm{witness} the unsatisfiability of a set of linear
inequalities. This feature is formalized by \introduceterm{Farkas'
  Lemma}: as it happens often with fundamental results, there are
several ways to state Farkas' Lemma.

\begin{lemma}[Farkas' Lemma for linear programming] 
  A set of linear inequalities $ Ax \leq b $ is unsatisfiable if and
  only if there exists a positive vector $ y \geq 0$ such that $
  y^{T}A = 0$ and $y^{T}b = -1$.
\end{lemma}

\begin{marginfigure}
  % \introduceterm{Proof complexity} interpretation. Later we will
  % discuss a little the proof complexity of unsatisfiability:  namely
  % how large and how hard to find is a proof that a set of constraints
  % is unsatisfiable of that a function cannot be optimized pass a
  % certain threshold.
  Two inequalities $ a^{T}x \leq b $ and $c^{T}x \leq d$ entail any
  positive combination as a logical consequence. Thus we can
  design a proof system $\proofsystem{LP}$ for sets of linear
  inequalities with the following inference rule,
  \begin{equation*}
    \frac{a^{T}x \leq b \quad c^{T}x \leq d}{(\alpha a + \gamma c)x
      \leq (\alpha b + \gamma d)x  }
  \end{equation*}
  with $\alpha \geq 0, \gamma \geq 0$. Farkas' Lemma claims that such
  proof system can deduce the contradiction $ 0 \leq -1 $ from any
  unsatisfiable set of linear inequalities.
\end{marginfigure}

If the linear program represents an optimization problem we can even
take positive combinations to prove bounds on the function to be
optimized. We start with the \introduceterm{primal} program that asks
to maximize $c^{T}x$ under constraints $ Ax \leq b$ and $ x\geq 0$ (notice
that we highlighted the non negativity constraints). Now consider a
non negative vector $ y^{T}$ such that $ y^{T}A \geq c^{T}$. For every
feasible solution $ x $ of the primal program it holds that
\begin{equation}
  c^{T} x \leq y^{T}Ax \leq y^{T} b\tag{weak duality}\label{eq:weak_duality}.
\end{equation}
Thus the solution $ y\geq 0 $ witnesses the \stressterm{upper bound} $
b^{T}y $ to the maximum achievable in the primal
program. \stressterm{How tight is such upper bound?} We can answer
that by looking at \introduceterm{dual}
program~\eqref{eq:dualprogram}. Notice also that the dual
of~\eqref{eq:dualprogram} is~\eqref{eq:primalprogram}.


\begin{figure}
\begin{minipage}[t]{0.5\textwidth}%
\begin{alignat}{2}
  \maximize c^{T}x\notag\\
\label{eq:primalprogram}\subjectto Ax \leq b \tag{P}\\
  \varpositive{x} \notag
\end{alignat}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}%
\begin{alignat}{2}
  \minimize b^{T}y \notag\\
  \label{eq:dualprogram}  \subjectto A^{T}y \geq c \tag{D}\\
  & \varpositive{y}\notag
\end{alignat}
\end{minipage}
\end{figure}

Farkas' Lemma gives a complete solution of the decision case, but
Farkas' Lemma also implies the \introduceterm{duality theorem}, which
basically claims that there are solutions of the dual program that
witness tight bounds for the primal program (and vice versa).

\begin{marginfigure}
  Theorem~\ref{thm:lp-duality} claims that proof system
  $\proofsystem{LP}$ can prove all valid linear inequalities. In proof
  theoretic jargon:  Farkas' Lemma is the completeness of
  $\proofsystem{LP}$, while Theorem~\ref{thm:lp-duality} is the 
  implicational completeness of $ \proofsystem{LP}$.
\end{marginfigure}

\begin{theorem}[Duality theorem]\label{thm:lp-duality}
  Consider the linear programs~\eqref{eq:primalprogram}
  and~\eqref{eq:dualprogram}, exactly one of the following holds
  \begin{itemize}
    \item neither~\eqref{eq:primalprogram} nor~\eqref{eq:dualprogram} have a
    feasible solution;
    \item program~\eqref{eq:primalprogram} has solutions with
    arbitrarily large values, and program~\eqref{eq:dualprogram} is
    unsatisfiable;
    \item program~\eqref{eq:dualprogram} has solutions with
    arbitrarily small values, and program~\eqref{eq:primalprogram} is
    unsatisfiable;
    \item both~\eqref{eq:primalprogram} and~\eqref{eq:dualprogram}
    have optimal solutions. Let $ x^{*}$ and $ y^{*}$ such solutions, then
    \begin{equation*}
      c^{T} x^{*}=b^{T}y^{*}.
    \end{equation*}
  \end{itemize}
\end{theorem}

\section{Complexity of Linear programming}

Deciding the satisfiability of a set of linear inequalities is clearly
in \NP.
% 
Farkas' Lemma and Duality Theorem show that deciding its
unsatisfiability is also in \NP, thus the problem is in $ \NP \cap \coNP
$. But actually there are well known efficient algorithms for this
problem. All of them are based the geometric interpretation of the
system of linear inequalities as a convex polyhedron.
\begin{itemize}
  \item The \introduceterm{simplex method} is the first algorithm for
  linear programming and has been invented by Dantzig (1947). It does
  not run in polynomial time, but it is quite fast in practice. The
  idea is to walk on the edges of the polyhedron induced by the linear
  program, in order to reach the vertex with optimal value.
  \item The first polynomial time algorithm for linear programming is
  based on the \introduceterm{ellipsoid method} (Khachyian, 1979). The
  simplex method is much faster in practice, but of course the
  ellipsoid method has great theoretical value, and runs under more
  general conditions. For example it allows to optimize over super
  polynomial size linear programs in polynomial time, assuming the
  program has an efficient \introduceterm{separator}\footnote{A
    separator is an oracle that, given a point $x$ outside the set
    of feasible solutions $F$, outputs an hyperplane separating $ x$
    from $ F$.}.  The algorithm assumes that the set of feasible
  solutions is in a 0-centered ball of large enough radius, and that
  it completely contains some ball of range $ \epsilon $.
  %
  The algorithm checks whether the center of the ball is feasible. If
  it is not then the separator oracle suggests which half of the ball
  to save and which to forget. A new ellipsoid of smaller volume
  ``encircles'' the useful half.  The idea is to encircle the set of
  feasible solutions with smaller and smaller ellipsoids, until either
  the center of the last ellipsoid is a feasible solution or the
  ellipsoid is so small that cannot contain a ball of range $ \epsilon
  $.
  
  \item Since the ellipsoid method is so inefficient in practice, the
  simplex method is usually preferred. The advent of
  \introduceterm{interior point methods} changed the scenario,
  providing a family of polynomial time algorithms which are fast in
  practice. The first member of this family is due to Karmakar (1984):
  his algorithm is based on the idea of exploring the space of
  solutions by walking through the interior of the polytope. The
  original algorithm was not faster than the simplex method but
  nowadays there are competitive implementations and variants. We must
  observe that interior point methods were already employed in the
  context of non-linear programming way before Karmakar algorithm.
\end{itemize}

\section{Integer programs and Linear relaxations}

Most problems in computer science require solutions that are discrete,
integer or $ \{0,1\} $. It is not possible express such constraints as
linear inequalities, so we need a new formalism called
\introduceterm{integer programming}. An integer program may have the
form

\begin{alignat*}{2}
  \maximize  c^{T}x\\
  \subjectto Ax \leq b\\
  & \varboolean{x_{i}}\text{\ or\ }\vardomain{x_{i}}{\ZZ}.
\end{alignat*}

It is pretty clear that integer programs are in $ \NP $, and that they
are expressive enough to encode $\NP$-complete problems, thus integer
programming is $\NP$-complete.
%
With a \introduceterm{linear relaxation} we trade expressibility
for efficiency: we drop all non-linear
constraints and we substitute them with fractional ones (\eg, $
x_{i}\in \ZZ $ becomes $ x_{i}\in\RR $ and $ x_{i} \in\{0,1\} $
becomes $ 0\leq x_{i}\leq 1 $).

\begin{example}[\introduceterm{Maximum weighted bipartite matching}]%
  \label{eg:bipartitematching}
  Consider the bipartite graph $G=(L,R,E)$, where $L$ and $R$ are two
  disjoint sets of vertices and $ E\subseteq L\times R $, and consider
  a weight function $\functionsignature{w}{E}{\RR} $.  The Maximum
  weighted bipartite matching problem looks for the matching of
  largest weight in the bipartite graph, and we can represent it with
  an integer program
  \begin{alignat*}{3}
    \maximize \sum_{e\in E} w(e) x_{e}\\
    \subjectto \sum_{e \ni v} x_{e} =1 \quad \text{for each $v\in L\cup R$}\\
    & x_{e} \in\{0,1\}.
  \end{alignat*}
  This is one of the few cases for which the integer program and its linear
  relaxation have the \stressterm{same optimum value}.
\end{example}

The reason for the tightness of the linear relaxation of
Example~\ref{eg:bipartitematching} is in the following property of the
matrix of constraints.

\begin{marginfigure}
  The criterion of total unimodularity given in
  Definition~\ref{def:tum} is not efficiently verifiable. Instead
  there are other sufficient criteria which can be verified in
  polynomial time.
\end{marginfigure}

\begin{definition}\label{def:tum}
  A matrix $ M $ is called \introduceterm{totally unimodular} if every
  square submatrix of $ M $ has determinant in $\{-1,0,1\}$.
\end{definition}

\begin{theorem}
  Consider the polyhedron $ P=\{x | Ax=b, x\geq 0\}$.  If $ A $ is
  totally unimodular and $ b $ is integer then the vertices of $ P$
  are all integers.
\end{theorem}

The previous theorem implies that an integer program with total
unimodular matrix of constraints can be efficiently solved by solving
its linear relaxation.


\section{Integrality gaps and Rounding}

In the case of Example~\ref{eg:bipartitematching} we saw that the
integer program and its linear relaxation have the same optimum, but
this is not the case in general. Actually the gap between the integer
and the fractional optimum can be very large.\footnote{%
  Consider the problem of computing the maximum independent set of a
  graph $ G=(V,E) $. In this case we want to maximize the objective
  function $ \sum_{v\in V} x_{v} $ under the assumption that $
  x_{u}+x_{v}\leq 1 $ for every pair $ \{u,v\}\in E$ for variables $
  \varboolean{x_{v}}$. If we consider the \stressterm{complete graph}
  then the maximum independent set has size at most 1, but the linear
  relaxation has a solution of value $ |V|/2 $, which is the vector $
  (\frac{1}{2},\frac{1}{2},\ldots,\frac{1}{2})$.}

For an optimization problem we may consider $\sOPT$, the integer
optimum, and $ \sFRAC$, which is the optimum of the relaxation. A
strategy to solve integer program is to relax it in some way, and to
\introduceterm{round its fractional solution} in order to get an
integer feasible solution. The rounding technique is strongly
dependent on the particular optimization problem and it is out of the
scope of this lecture.
%
Let $ \sROUND $ denotes such solution, then for (say) a maximization problem
we have
\begin{equation}
  \label{eq:round_opt_frac}
  \sFRAC \geq \sOPT \geq \sROUND
\end{equation}

\begin{definition}
  Given an integer program and its relaxation, the ratio 
\begin{equation}
  \frac{\sOPT}{\sFRAC}
\end{equation}
is the \introduceterm{integrality gap} of the relaxation.
\end{definition}

Very often the analysis of an approximation algorithm for a
maximization problem is achieved by determining the ratio between the
fractional optimum and its rounded value, $ \frac{\sROUND}{\sFRAC}
$. The latter is bounded from above by the integrality
gap\footnote{For a minimization problem we get $ \sFRAC \leq \sOPT
  \leq \sROUND $, so the integrality gap actually lower bounds $
  \frac{\sROUND}{\sFRAC}$.}.

\section{Improving the linear relaxations}

We discussed the gap between an integer program and its linear
relaxation, which is a key concept in the analysis of approximation
algorithms. To get a tighter relaxation we can add
\stressterm{additional inequalities} to the initial program. Such
inequalities must be valid for integer solutions but may still shave
off some of the fractional solutions.

Given an arbitrary linear program let $ \sol{P}$ be the convex hull of
its feasible solutions, and $ \sol{P}_{I} $ the convex hull of its
integer solutions. Of course $ \sol{P}_{I}\subseteq\sol{P} $. The idea
is to find a way to efficiently identify a convex set $ \sol{E} $ such
that $ \sol{P_{I}}\subseteq \sol{E} \subseteq\sol{P}_{I} $.  If the
gap between the optimal solutions in $ \sol{E} $ and the ones in $
\sol{P}_{I} $ is strictly smaller than the integrality gap of the
original linear program, then we have a better handle to solve the
combinatorial problem. Even better if $ \sol{E}=\sol{P}_{I} $: in this
case there is an efficient way to find an optimal solution for the
problem.
%
If $ \PTIME\neq\NP $ there is not general way to obtain $\sol{P}_{I} $
by adding just a polynomial number of linear inequalities.

\introduceterm{Cutting planes:} Gomory\cite{gomory1958outline}
introduces a way to cut fractional solutions by noticing that if $
a^{T}x \leq b $ is a valid inequality and $ a $ is an integer vector,
then
\begin{equation}
  a^{T}x \leq \lfloor b \rfloor
\end{equation}
is valid over integer solutions (but it is not over fractional
ones). This method has been employed effectively in integer
programming software.

\introduceterm{Extended formulation}: here the idea is to add new
variables and add linear constraints on them in such a way that the
projection of the extended polytope $\sol{P}_{E}$ on the original
variables is closer or even equal to $ \sol{P}_{I} $.

Such extended formulation is ``problem dependent'' and there are very
nice connections between the number of inequalities needed to capture
$ \sol{P}_{I} $ and communication complexity. Fiorini
et.al.\cite{fiorini2012linear} proved that there is no polynomial size
extended formulation for the canonical integer programming
representation of the \introduceterm{traveller salesman problem}.

In this course we deal with systematic ways to improve the relaxation
of the integer programs. In this lecture we are going to see some
techniques specific to linear programming.

\section{\Lovasz-\Schrijver\ hierarchy}

Let us just focus on feasibility problems (\ie, no function to be
optimized). Then the point is to get close to a representation of $
\sol{P}_{I} $ in order to determine whether it is empty or not. We now
that if we could efficiently decide feasibility under
\introduceterm{quadratic inequalities} we could easily solve integer
program over boolean variables, since $ x_{i}\in\{0,1\} $ is
equivalent to $ x^{2}_{i} - x_{i} =0 $.

The \Lovasz-\Schrijver\cite{lovasz1991cones} linear relaxation is
based on the idea of using quadratic inequalities to determine new
\stressterm{linear inequalities} which are valid for the integer
solutions but not for the fractional ones. Once the new linear
inequalities are in place, the quadratic inequalities are forgotten.
The resulting linear program is tighter than the initial one and we
can optimized against it.

The simplest way to describe \Lovasz-\Schrijver integer programming
relaxation is to interpret it as a proof system. Start with its linear
relaxation $ Ax \geq b, 0 \leq x \leq 1$, and call $ \sol{P} $ the set
of solutions of such inequalities\footnote{We focus on integer
  programs with boolean solutions here, thus the presence of
  inequalities $ 0 \leq x \leq 1 $ in the relaxation by
  default. Thus we can always assume we deal with polytopes here.}.

Every inequality $ \sum_{j}a_{i,j}x_{j} - b_{i} \leq 0$ is an axiom of
the proof system, together with axioms $ - x_{i} \leq 0$, $ x_{i} - 1
\leq 0$ and $ x_{i} - x^{2}_{i} \leq 0$.  The system allows to multiply a
linear inequality by a variable or by the complement of a variable
\begin{equation}\label{eq:lsmultiplication}
\AxiomC{$\sum_{j}c_{j} x_{j} - d \leq 0$}
\UnaryInfC{$ \sum_{j}c_{j}(x_{j}x_{i}) - d x_{i} \leq 0$}
\DisplayProof
\qquad
\AxiomC{$ \sum_{j}c_{j} x_{j} - d \leq 0$}
\UnaryInfC{$ \sum_{j}c_{j}x_{j}(1-x_{i}) - d (1-x_{i}) \leq 0$}
\DisplayProof,
\end{equation}

and to infer positive combinations of known inequalities

\begin{figure*}
\begin{equation}
\RightLabel{\ assuming $\alpha, \beta \geq 0$.}
\AxiomC{$c+\sum_{j}c_{j} x_{j} + \sum_{\{i,j\}} c_{\{i,j\}}x_{i}x_{j}\leq 0$}
\AxiomC{$d+\sum_{j}d_{j} x_{j} + \sum_{\{i,j\}} d_{\{i,j\}}x_{i}x_{j}\leq 0$}
\BinaryInfC{%
$\alpha (c+\sum_{j}c_{j} x_{j} + \sum_{\{i,j\}} c_{\{i,j\}}x_{i}x_{j})+
\beta (d+\sum_{j}d_{j} x_{j} + \sum_{\{i,j\}} d_{\{i,j\}}x_{i}x_{j}) \leq 0$}
\DisplayProof
\end{equation}
\end{figure*}

Quadratic inequalities can be used to derive new linear inequalities,
which in turn can be multiplied again. We will keep track on how many
multiplication steps are needed to derive a particular
inequality. This is the \introduceterm{rank of an inequality} in
\Lovasz-\Schrijver.

\begin{definition}[Rank of \Lovasz-\Schrijver]
  Consider a derivation of an inequality in \Lovasz-\Schrijver. The
  \introduceterm{rank of derivation} of an axiom is $ 0 $; the rank of
  a multiplication step is one plus the rank of the premise; the rank
  of a positive sum is the maximum among the rank of the premises.
  The \introduceterm{rank of an inequality} is the smallest among the
  rank all possible derivations of that inequality. The
  \introduceterm{rank of a point} is the smallest among the rank of
  all linear inequalities that are falsified by that point. The rank
  of the empty polytope is the rank of the inequality $ 1 \leq 0 $.
\end{definition}

The set of solution of all linear inequalities of rank $ 0 $ is
denoted as $ \sol{P}_{0} $, and is exactly equal to $ \sol{P} $. The
polytope characterized by the points compatible with all linear
inequalities of rank $ t $ is denoted as $ \sol{P}_{t} $. It is
important to stress that while derivations are allowed to use
quadratic inequalities, the polytopes are just defined by the linear
ones. It holds that

\begin{equation}
  \sol{P}=\sol{P_{0}} \supseteq \sol{P}_{1} \subseteq \cdots
  \supseteq \sol{P}_{n-1} \subseteq \sol{P}_{n}=\sol{P}_{I}.
\end{equation}

where the last equation is proved in\cite{lovasz1991cones}.

\begin{theorem}[\Lovasz-\Schrijver, 1991]
  The polyhedron $ \sol{P}_{n} $ is equal to the convex hull of
  the integer points  $ \sol{P}_{I} $
\end{theorem}

The original and most popular definition of \Lovasz-\Schrijver
relaxation is by a geometric interpretation, and it is also the original definition. Given the a
polytope $ \sol{P} $ defined on points in $ \RR^{n} $. We consider a
matrix variable $X$ over $ \RR^{(n+1)\times(n+1)} $ with row and
column indices going from $ 0 $ to $ n $, which must satisfy the
following inequalities: $ x_{i0} = x_{0i} = x_{ii} $ for all $ i \in
[n] $; $ x_{ij}=x_{ji} $ for every $ i,j \in [n]$; if $
\sum_{j}a_{j}x_{j} - b \leq 0 $ then $ X $ must satisfy 
\begin{equation*}
  \sum_{j}a_{j}x_{ij} - b_{x_{i0}} \leq 0
\end{equation*}
for every $ i \in \{0\}\cup[n]$. Let's call $ \sol{M} $ the set of
points satisfying these inequalities.

This matrix satisfies every inequality of rank 1 derived in
\Lovasz-\Schrijver from $\sol{P}$, if the quadratic terms $ x_{i}x_{j}
$ are mapped to $ x_{ij} $, if $ x_{i} $ are mapped to $ x_{0i} $ and
each degree zero term is multiplied by $ x_{00} $. We
define\footnote{this is often called the
  \introduceterm{\Lovasz-\Schrijver operator}.}  $ N(\sol{P}) $ as the
projection over the variable $ x_{00},x_{01},\ldots, x_{0n} $, with $
x_{00}=1 $. So it is clear that $ N(\sol{P})=\sol{P}_{1} $ and that $
N(\sol{P}_{t})=\sol{P}_{t+1} $. Notation $ N^{t}(\sol{P}) $ is the $ t
$th iteration of the operaton $ N $ over $ \sol{P} $.

\Lovasz-\Schrijver is a so called \introduceterm{Lift and project}
method to improve on the linear relaxation. The reason for this name
is clear: first the linear program is augmented to $ \approx n^{2} $
variables, and then is projected again to the original
variables. Moving to the larger space allows to get tighter
inequalities in the original space.

\begin{theorem}\cite{lovasz1991cones}
  Let be $\sol{P}$ a polytope described by $ n^{O(1)} $
  inequalities. It is possible to optimize any linear function over $
  \sol{P}_{t} $ in time $ n^{O(t)} $.
\end{theorem}
\begin{proof}[Proof Sketch]
  In order to optimize over a linear program using the ellipsoid
  algorithm it is sufficient to have a weak separator
  oracle. In\cite{lovasz1991cones} they show that a weak separator
  oracle for any polytope $ \sol{Q} $ gives a weak separator oracle
  for $ N(\sol{P}) $ which works in polynomial time.
\end{proof}

\section{Sherali-Adams hierarchy}

Here we start with a set of polynomial inequalities in variables $
x_{1}\ldots x_{n} $ over $ \{0,1\} $.
\begin{equation}
  p_{1}\geq 0 , p_{2}\geq 0, \ldots , p_{m} \geq 0. 
\end{equation}

A Sherali-Adams proof of a polynomial inequality $p\leq 0$ over variables is an
equation of the form
\begin{equation}
 \label{eq:sa_inference}
 \sum_{1\leq l\leq m} p_{l} g_{l} + \sum_{i} (x^{2}_{i} -x_{i}) h_{i}
 + g_{0}= p
\end{equation}

where each $ g_{l} = \alpha_{l} \prod_{i\in I_{l}} x_{i} \prod_{j \in
  J_{l}}(1-x_{j}) $ with $ \alpha_{l}\geq 0 $ and $ I_{l} \cap
J_{l}=\emptyset $; and each $ h_{i} $ is an arbitrary polynomial.

The \introduceterm{rank of the Sherali-Adams}%
\footnote{there are other definitions of rank in literature. For
  example\cite{laurent01} define the Sherali-Adams rank as the degree
  of the polynomials $ g_{l} $. In this way you get than Sherali-Adams
rank matches \Lovasz-\Schrijver rank.} 
%
proof is equal the the
maximum degree among the polynomials $ g_{0} $, $ g_{l}p_{l} $, $
h_{i}(x^{2}_{i}- x_{i}) $. A refutation of a set of polynomials is a
Sherali-Adams proof of $ -1 \geq 0 $.

While Sherali-Adams is a natural proof system for non-linear
inequalities, the linear case is the one we are interested to.

If $ p_{l}\geq 0 $ are linear inequalities, then they define a
polytope since we are always under the assumption that $0\leq
x_{i}\leq 1 $. Let's call this polytope $ \sol{P} $. Then we can
define $ S_{t}(\sol{P}) $ as the polytope obtained by all linear
inequalities obtained through a proof of rank at most $ t $. It is
clear from the definition that $ N^{t}(\sol{P}) \supseteq
S_{t+1}(\sol{P}) \supseteq \sol{P}_{I} $, and that 
\begin{equation}
  \sol{P}=S_{0}(\sol{P}) \supseteq S_{2}(\sol{P}) \supseteq
  S_{n+1}(\sol{P}) = \sol{P}_{I}.
\end{equation}


Sherali-Adams can be seen as a Lift-and-Project relaxation as
well. Consider $ Y\in \RR^{s} $ where $ s=\binom{n}{\leq t} $,
where every coordinate of $ Y $ is naturally indexed by a subset
of $ [n] $ of size at most $ t $.

Map every monomial $m$ over variables $ x_{i} $ into the variable $
Y_{I(m)}$ where $ I(m) $ is the set of indices of the variables in $ m
$.  For example
\begin{equation*}
x^{2}_{2} x_{3}x_{5} \mapsto Y_{\{2,3,5\}} 
\end{equation*}

This map translate any polynomial inequality of degree at most $ t
$ into a linear inequality of over $ \RR^{s} $. Take $ \sol{E} $ to be
the set of all vectors $ Y $ that satisfy all inequalities obtained by
applying this translation to every $ g p_{l} \geq 0$ of degree at most
$t$ and $ g \geq 0 $ for every $ g $ of degree $ g $. The polytope $
\sol{E} $ satisfies all linear inequalities derivable in Sherali-Adams
proofs of rank at most $ t $, after projecting over $
(y_{\emptyset},y_{1},\ldots,y_{n})$.


\stressterm{Efficiency of SA}: if the linear system has length $
n^{O(1)} $ then the polytope $ S_{t}(\sol{P}) $ can be described by $
n^{O(t)} $ linear inequalities over the $ Y_{S} $ variables. Since
this is a linear system, both optimization and feasibility can be
computed in time $ n^{O(t)} $.

\section{An example of Sherali-Adams relaxation}

Consider the maximum independent set integer program for a graph $
G=(V,E)$.
\begin{alignat}{2}
  \maximize \sum_{v\in V} x_{v}\notag \\
  \subjectto x_{u} + x_{v} - 1\leq 0 \qquad \text{for $ \{u,v\}\in E $ }\\
  & 0 \leq x_{u}\leq 1.\notag
\end{alignat}

The Sherali-Adams relaxation of rank $t$ is

\begin{alignat*}{2}
  \maximize \sum_{v\in V} Y_{\{v\}}\notag \\
  \subjectto \sum_{T'\subseteq T} (-1)^{T'} \cdot 
  \left[ 
    Y_{S\cup T'\cup \{u\} } + Y_{S\cup T'\cup \{v\} } - Y_{S\cup T'}
  \right]
  \leq 0 \\ 
  &\qquad \text{for $ \{u,v\}\in E $,$ |S\cup T\cup \{u\}| \leq
    t,|S\cup T\cup \{v\}|\leq t $}\\
  \\
  &0 \leq \sum_{T'\subseteq T} (-1)^{T'} Y_{S\cup T'\cup \{u\} } \leq
  \sum_{T'\subseteq T} (-1)^{T'} Y_{S\cup T' }\\
  &\qquad \text{for $ \{u,v\}\in E $, $|S\cup T\cup \{u\}| \leq
    t,|S\cup T\cup \{v\}|\leq t $  }
\end{alignat*}


% ------------------------- EPILOGUE ------------------------------
\bibliography{soscourse}
\bibliographystyle{alpha}

\end{document} 


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
