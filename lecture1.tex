% Copyright (C) 2014 by Massimo Lauria
% 
% Created   : "2014-01-07, Tuesday 17:01 (CET) Massimo Lauria"
% Time-stamp: "2014-01-10, 19:03 (CET) Massimo Lauria"
% Encoding  : UTF-8

% ---------------------------- USER DATA ------------------------------
\def\DataTitle{1. Linear relaxations of integer programs.}
\def\DataTitleShort{Linear programming}
\def\DataDate{27 January, 2014}
\def\DataDocname{Lecture 1 --- \DataDate}
\def\DataLecturer{Massimo Lauria}
\def\DataScribe{Massimo Lauria}
\def\DataKeywords{integer programming, linear programming,
  \Lovasz-\Schrijver, Sherali-Adams, }

\def\DataAbstract{%
  We can express combinatorial problems using integer programs, but
  since we can't solve them, we consider relaxed linear programs and
  we round fractional solutions to integer. We consider hierarchies of
  linear programs and discuss the quality of the corresponding
  solutions.}


% ---------------------------- PREAMBLE -------------------------------
\documentclass[a4paper,twoside,justified]{tufte-handout}
\usepackage{soscourse} % this is a non standard package
\begin{document} 
% --------------------------- DOCUMENT --------------------------------

This lecture is a sort of scaled down demo of the rest of the
course. Here we are see that we can express decisions and optimization
problems by the means of \introduceterm{integer programs}. This translation in
even possible for $ \NP $-hard problems, thus there is not efficient
algorithm to solve integer programs unless $\PTIME=\NP$, which is
considered by many to be very unlikely\footnote{Every hardness result
  we will see during the course won't rely on any unproved assumption.}.

In any case there are no \stressterm{known} efficient algorithm to
solve integer programs, and a viable strategy is to
\introduceterm{relax} the integer program to something more
manageable: for example a liner program.

The most naive way to do that is to transform the integrality
constraints into fractional linear constraints, \eg $ x \in\{0,1\} $
into $ 0 \leq x \leq 1 $, and leave the other constraints as they
are\footnote{We can assume that all such constraints are affine,
  namely of the three forms
  \begin{equation*}
    \sum_{i}a_{i}x_{i} \leq b \quad   \sum_{i}a_{i}x_{i} \geq b \quad \sum_{i}a_{i}x_{i} = b
  \end{equation*}
  for $a_i$ and $ b $ in \RR}.

Once we relax the integer program we have a lot of new fractional
solution that are not allowed in the integer one. For example consider
the program that characterizes the maximum independent sets of graph $
K_{3} $, \ie, the triangle.

\begin{alignat}{2}
  \maximize x_{1} + x_{2} + x_{3} \notag\\
  \subjectto   x_{1} + x_{2} \leq 1 \notag\\
             & x_{2} + x_{3} \leq 1 \label{eq:triangle-ind-set}\\
             & x_{1} + x_{3} \leq 1 \notag\\
             & \varboolean{x_{1}},\ \varboolean{x_{2}},\ \varboolean{x_{3}}.\notag
\end{alignat}

Its \introduceterm{integer optimum} is 1 since no two variables can be
both 1, but if we relax the constraints and allow $ \varbounded{x_{i}}
$, then the linear program has a \introduceterm{fractional optimum}
of $ \frac{3}{2} $, by setting all variables to $\frac{1}{2}$.

Most of the course will study systematic techniques to improve the
relaxation, adding variables and inequalities in order to
\stressterm{cut away} feasible fractional solutions without changing
the set of integer solutions. The quality and the complexity of such
techniques is controlled by a parameter called \introduceterm{rank}:
larger the rank, less fractional solution remain.

\section{Linear programming}

The most classic and studied optimization formalism is
\introduceterm{linear programming}, which is illustrated by books as
\cite{matousek2007understanding}: we want to optimize (either minimize
of maximize) a linear function $ \sum_{i} c_{i}x_{i} $ over the set of
variables $ \{x_{i}\}^{n}_{i=1} $, which are constrained to satisfy a
set of linear inequalities and linear equations. Without loss of
generality we can assume a linear program to have one of the following
forms\footnote{While we will discuss the left form more often, the
  right one is also very common and is called the
  \introduceterm{standard form}. The standard form is particularly
  useful in the simplex implementation.}.

\begin{figure}
\begin{minipage}[t]{0.5\textwidth}
\begin{alignat*}{2}
  \minimize c^{T}x \\
  \subjectto Ax \leq b
\end{alignat*}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{alignat*}{2}
  \minimize c^{T}x \\
  \subjectto Ax = b\\
  & x \geq 0
\end{alignat*}
\end{minipage}
\end{figure}


In order to optimize a linear program it is of course necessary to
understand is the program is \stressterm{feasible}, \ie if the is a
value of $ x $ that satisfies, for example the linear inequalities $Ax
\leq b$. Thus linear programs describe both \stressterm{decision} and
\stressterm{optimization} problems.

The striking feature of linear program is the possibility to
\stressterm{witness} the unsatisfiability of a set of linear
inequalities. This feature is formalized by \introduceterm{Farkas
  Lemma}: as it happens often with fundamental results, there are
several ways to state Farkas Lemma.

\begin{lemma}[Farkas Lemma]
  A set of linear inequalities $ Ax \leq b $ is unsatisfiable if and
  only if there exists a positive vector $ y \geq 0$ such that $
  y^{T}A = 0$ and $y^{T}b = -1$.
\end{lemma}

\begin{marginfigure}
  % \introduceterm{Proof complexity} interpretation. Later we will
  % discuss a little the proof complexity of unsatisfiability:  namely
  % how large and how hard to find is a proof that a set of constraints
  % is unsatisfiable of that a function cannot be optimized pass a
  % certain threshold.
  Given two inequalities $ a^{T}x \leq b $ and $c^{T}x \leq d$
  then any positive combination is a semantic consequence. Thus we can
  design a proof system for sets of linear inequalities with the
  following inference rule,
  \begin{equation*}
    \frac{a^{T}x \leq b \quad c^{T}x \leq d}{(\alpha a + \gamma c)x
      \leq (\alpha b + \gamma d)x  }
  \end{equation*}
  with $\alpha \geq 0, \gamma \geq 0$. Farkas Lemma states that such
  proof system can always deduce a contradiction $ 0 \leq -1 $ from a
  set of unsatisfiable linear inequalities.
\end{marginfigure}

Deciding the satisfiability of a set of linear inequalities is clearly
in \NP\@.  Farkas Lemma shows that also deciding its unsatisfiability
is in \NP, thus the problem is in $ \NP \cap \coNP $. But it is
actually well known that there are efficient algorithms for this problem.
\begin{itemize}
  \item The \introduceterm{simplex method} is the first algorithm
  invented for linear programming and has been invented by (Dantzig,
  1947). It does not run in polynomial time, but it is quite fast in
  practice.
  \item The first polynomial time algorithm for linear programming is
  based on the \introduceterm{ellipsoid method}. Despite being
  polynomial time, the simplex is much faster in practice.

\end{itemize}




\begin{marginfigure}
\begin{example}
  Consider the problem of finding a perfect matching in the graph,
  which is expressed by the following program.
  \begin{alignat*}{2}
    e_{11} + e_{12} & \leq 1\\
    e_{22} + e_{}
  \end{alignat*}
\end{example}
\end{marginfigure}

\section{Integer programs and Linear relaxations}


\section{Integrality gaps}


\section{Improving the linear relaxations}


\section{\Lovasz-\Schrijver\ hierarchy}

\subsection{Rank lower bounds}

\section{Sherali-Adams hierarchy}

\subsection{Rank lower bounds}


% ------------------------- EPILOGUE ------------------------------
\bibliography{theoryofcomputing,soscourse}
\bibliographystyle{alpha}

\end{document} 


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
