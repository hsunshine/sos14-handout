% Copyright (C) 2014 by Massimo Lauria
% 
% Created   : "2014-01-07, Tuesday 17:01 (CET) Massimo Lauria"
% Time-stamp: "2014-01-16, 22:33 (CET) Massimo Lauria"
% Encoding  : UTF-8

% ---------------------------- USER DATA ------------------------------
\def\DataTitle{1. Linear relaxations of integer programs.}
\def\DataTitleShort{Linear programming}
\def\DataDate{27 January, 2014}
\def\DataDocname{Lecture 1 --- \DataDate}
\def\DataLecturer{Massimo Lauria}
\def\DataScribe{Massimo Lauria}
\def\DataKeywords{integer programming, linear programming,
  \Lovasz-\Schrijver, Sherali-Adams, }

\def\DataAbstract{%
  We can express combinatorial problems using integer programs, but
  since we can't solve them, we consider relaxed linear programs and
  we round fractional solutions to integer. We consider hierarchies of
  linear programs and discuss the quality of the corresponding
  solutions.}


% ---------------------------- PREAMBLE -------------------------------
\documentclass[a4paper,twoside,justified]{tufte-handout}
\usepackage{soscourse} % this is a non standard package
\begin{document} 
% --------------------------- DOCUMENT --------------------------------

This lecture is a sort of scaled down demo of the rest of the
course. Here we are see that we can express decisions and optimization
problems by the means of \introduceterm{integer programs}. This translation in
even possible for $ \NP $-hard problems, thus there is not efficient
algorithm to solve integer programs unless $\PTIME=\NP$, which is
considered by many to be very unlikely\footnote{Every hardness result
  we will see during the course won't rely on any unproved assumption.}.

In any case there are no \stressterm{known} efficient algorithm to
solve integer programs, and a viable strategy is to
\introduceterm{relax} the integer program to something more
manageable: for example a liner program.

The most naive way to do that is to transform the integrality
constraints into fractional linear constraints, \eg $ x \in\{0,1\} $
into $ 0 \leq x \leq 1 $, and leave the other constraints as they
are\footnote{We can assume that all such constraints are affine,
  namely of the three forms
  \begin{equation*}
    \sum_{i}a_{i}x_{i} \leq b \quad   \sum_{i}a_{i}x_{i} \geq b \quad \sum_{i}a_{i}x_{i} = b
  \end{equation*}
  for $a_i$ and $ b $ in \RR}.

Once we relax the integer program we have a lot of new fractional
solution that are not allowed in the integer one. For example consider
the program that characterizes the maximum independent sets of graph $
K_{3} $, \ie, the triangle.

\begin{alignat}{2}
  \maximize x_{1} + x_{2} + x_{3} \notag\\
  \subjectto   x_{1} + x_{2} \leq 1 \notag\\
             & x_{2} + x_{3} \leq 1 \label{eq:triangle-ind-set}\\
             & x_{1} + x_{3} \leq 1 \notag\\
             & \varboolean{x_{1}},\ \varboolean{x_{2}},\ \varboolean{x_{3}}.\notag
\end{alignat}

Its \introduceterm{integer optimum} is 1 since no two variables can be
both 1, but if we relax the constraints and allow $ \varbounded{x_{i}}
$, then the linear program has a \introduceterm{fractional optimum}
of $ \frac{3}{2} $, by setting all variables to $\frac{1}{2}$.

Most of the course will study systematic techniques to improve the
relaxation, adding variables and inequalities in order to
\stressterm{cut away} feasible fractional solutions without changing
the set of integer solutions. The quality and the complexity of such
techniques is controlled by a parameter called \introduceterm{rank}:
larger the rank, less fractional solution remain.

\section{Linear programming}

The most classic and studied optimization formalism is
\introduceterm{linear programming}, which is illustrated by books as
\cite{matousek2007understanding}: we want to optimize (either minimize
of maximize) a linear function $ \sum_{i} c_{i}x_{i} $ over the set of
variables $ \{x_{i}\}^{n}_{i=1} $, which are constrained to satisfy a
set of linear inequalities and linear equations. Without loss of
generality we can assume a linear program to have one of the following
forms\footnote{While we will discuss the left form more often, the
  right one is also very common and is called the
  \introduceterm{standard form}. The standard form is particularly
  useful in the simplex implementation.}.

\begin{figure}
\begin{minipage}[t]{0.5\textwidth}
\begin{alignat*}{2}
  \maximize c^{T}x\\
  \subjectto Ax \leq b
\end{alignat*}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{alignat}{2}\label{eq:lpforms}
  \maximize c^{T}x \notag\\
  \subjectto Ax = b\\
  & x \geq 0\notag
\end{alignat}
\end{minipage}
\end{figure}


In order to optimize a linear program it is of course necessary to
understand is the program is \stressterm{feasible}, \ie if the is a
value of $ x $ that satisfies, for example the linear inequalities $Ax
\leq b$. Thus linear programs can describe both \stressterm{decision} and
\stressterm{optimization} problems.

The striking feature of linear program is the possibility to
\stressterm{witness} the unsatisfiability of a set of linear
inequalities. This feature is formalized by \introduceterm{Farkas
  Lemma}: as it happens often with fundamental results, there are
several ways to state Farkas Lemma.

\begin{lemma}[Farkas Lemma]
  A set of linear inequalities $ Ax \leq b $ is unsatisfiable if and
  only if there exists a positive vector $ y \geq 0$ such that $
  y^{T}A = 0$ and $y^{T}b = -1$.
\end{lemma}

\begin{marginfigure}
  % \introduceterm{Proof complexity} interpretation. Later we will
  % discuss a little the proof complexity of unsatisfiability:  namely
  % how large and how hard to find is a proof that a set of constraints
  % is unsatisfiable of that a function cannot be optimized pass a
  % certain threshold.
  Given two inequalities $ a^{T}x \leq b $ and $c^{T}x \leq d$ then
  any positive combination is a semantic consequence. Thus we can
  design a proof system $\proofsystem{LP}$ for sets of linear
  inequalities with the following inference rule,
  \begin{equation*}
    \frac{a^{T}x \leq b \quad c^{T}x \leq d}{(\alpha a + \gamma c)x
      \leq (\alpha b + \gamma d)x  }
  \end{equation*}
  with $\alpha \geq 0, \gamma \geq 0$. Farkas Lemma states that such
  proof system can always deduce a contradiction $ 0 \leq -1 $ from a
  set of unsatisfiable linear inequalities.
\end{marginfigure}

If the linear program is used for optimization, it is interesting that
we can take positive combinations to prove bounds on the function to
be optimized. Start with the \introduceterm{primal} program that asks to minimize $c^{T}x$
under constraints $ Ax \leq b, x\geq 0$ (notice that we highlighted
the non-negativity constraints). Now consider a non negative vector $
y^{T}$ such that $ y^{T}A \geq c^{T}$ and such that $ y^{T}b = v$. For
every feasible solution $ x $ of the primal program it holds that 
\begin{equation}
  c^{T} x \leq y^{T}Ax \leq y^{T} b\tag{weak duality}\label{eq:weak_duality}
\end{equation}
Thus the solution $ y\geq 0 $ witnesses the \stressterm{upper bound} $
b^{T}y $ to the maximum achievable in the primal
program. \stressterm{How tight is such upper bound?} We can answer
that by looking at \introduceterm{dual}
program~\eqref{eq:dualprogram}. Notice also that the dual
of~\eqref{eq:dualprogram} is~\eqref{eq:primalprogram}.


\begin{figure}
\begin{minipage}[t]{0.5\textwidth}%
\begin{alignat}{2}
  \maximize c^{T}x\notag\\
\label{eq:primalprogram}\subjectto Ax \leq b \tag{P}\\
  \varpositive{x} \notag
\end{alignat}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}%
\begin{alignat}{2}
  \minimize b^{T}y \notag\\
  \label{eq:dualprogram}  \subjectto A^{T}y \leq c \tag{D}\\
  & \varpositive{y}\notag
\end{alignat}
\end{minipage}
\end{figure}

We saw that Farkas Lemma gives a complete solution of the decision
case. From the same Farkas Lemma it is possible to prove the
\introduceterm{duality theorem}, which basically claims that there are
solutions of the dual program that witness tight bounds on the primal
program (and viceversa).

\begin{marginfigure}
  As in the case of Farkas Lemma,
  Theorem claims that proof system $\proofsystem{LP}$ is complete in
  the sense that can prove all valid linear inequalities.
\end{marginfigure}

\begin{theorem}[Duality theorem]
  Consider the linear programs~\eqref{eq:primalprogram}
  and~\eqref{eq:dualprogram}, exactly one of the following happens
  \begin{itemize}
    \item Neither~\eqref{eq:primalprogram} nor~\eqref{eq:dualprogram} have a
    feasible solution.
    \item Program~\eqref{eq:primalprogram} has solutions with
    arbitrarily large value, and program~\eqref{eq:dualprogram} is
    unfeasible.
    \item Program~\eqref{eq:dualprogram} has solutions with
    arbitrarily small value, and program~\eqref{eq:primalprogram} is
    unfeasible.
    \item Both~\eqref{eq:primalprogram} and~\eqref{eq:dualprogram}
    have optimal solutions. Let $ x^{*}$ and $ y^{*}$ such solutions, then
    \begin{equation*}
      c^{T} x^{*}=b^{T}y^{*}.
    \end{equation*}
  \end{itemize}
\end{theorem}

\section{Complexity of Linear programming}

Deciding the satisfiability of a set of linear inequalities is clearly
in \NP\@.  Farkas Lemma and Duality Theorem shows that also deciding
its unsatisfiability is in \NP, thus the problem is in $ \NP \cap
\coNP $. But actually there are well known efficient algorithms for
this problem. All of them are based the geometric interpretation of
the system of linear inequalities as a convex polyhedron. 
\begin{itemize}
  \item The \introduceterm{simplex method} is the first algorithm
  invented for linear programming and has been invented by (Dantzig,
  1947). It does not run in polynomial time, but it is quite fast in
  practice. The idea is to walk on the edges of the polyhedron induced
  by the linear program, in order to reach the vertex with optimal
  value.
  \item The first polynomial time algorithm for linear programming is
  based on the \introduceterm{ellipsoid method} (Khachyian, 1979). The
  simplex is much faster in practice, but of course the ellipsoid has
  great theoretical value, and runs under more general conditions. For
  example super polynomial size linear programs, assuming the program
  has an efficient \introduceterm{separator}\footnote{A separator is
    an oracle that, given a point $x$ outside of the set of feasible
    solutions $F$, outputs an hyperplan separating $ p$ from $ F$.}.
  The algorithms assumes that the set of feasible solutions is in a
  0-centered ball of large enough radius, and completely
  contains some ball of range $ \epsilon $.
  %
  The algorithm check whether the center of the ball is feasible. If
  it is not then the separator oracle suggest which half of the ball
  to save and which to forget. A new ellipsoid of smaller volume
  ``encircles'' the useful half.  The idea is to encircle the set of
  feasible solutions with smaller and smaller ellipsoids, until either
  the center of the one ellipsoid is feasible or the ellipsoid is so
  small that cannot contain a ball of range $ \epsilon $.
  
  \item Since the ellipsoid method is so inefficient in practice, the
  simplex is usually preferred. The advent of \introduceterm{interior
    point methods} changed the scenario, providing a family of
  polynomial time algorithms which are fast in practice. The first
  member of such family is due to Karmakar (1984): his algorithm is
  based on the idea of exploring the space of solution walking through
  the interior of the polytope. The original algorithm was not faster
  than the simplex method but nowadays there are competitive
  improvements and variants. We must observe that interior point
  methods were already used in the context of non-linear programming
  way before than Karmakar algorithm.
\end{itemize}

\section{Integer programs and Linear relaxations}

Most problems in computer science require solutions that are discrete,
integer or $ \{0,1\} $. It is not possible express such constraints as
linear inequalities, so we need a new formalism called
\introduceterm{integer program}. An integer program may have the
following form.

\begin{alignat*}{2}
  \maximize  c^{T}x\\
  \subjectto Ax \leq b\\
  & \varboolean{x_{i}}\text{\ or\ }\vardomain{x_{i}}{\ZZ} 
\end{alignat*}

It is pretty clear that integer programs are in $ \NP $, and that they
are expressive enough to encode $\NP$-complete problems, thus Integer
programming is $\NP$-complete.
%
With a \introduceterm{linear relaxation} we trade the expressibility
of the program with and efficient solver. We drop all non-linear
constraints and we substitute them with fractional ones (\eg, $
x_{i}\in \ZZ $ becomes $ x_{i}\in\RR $ and $ x_{i} \in\{0,1\} $
becomes $ 0\leq x_{i}\leq 1 $).

\begin{example}[\introduceterm{Maximum weighted bipartite matching}]%
  \label{eg:bipartitematching}
  Consider the bipartite graph $G=(L,R,E)$, where $L$ and $R$ are two
  disjoint sets of vertices and $ E\subseteq L\times R $, and consider
  a weight function $\functionsignature{w}{E}{\RR} $.  The Maximum
  weighted bipartite matching problem looks for the largest matching
  in the bipartite graph, and we can encoding as follows.
  \begin{alignat*}{3}
    \maximize \sum_{e\in E} w(e) x_{e}\\
    \subjectto \sum_{e \ni v} x_{e} =1 \quad \text{for each $v\in L\cup R$}\\
    & x_{e} \in\{0,1\}
  \end{alignat*}
  This is one of the few cases for which the integer program and its linear
  relaxation have the \stressterm{same optimum value}.
\end{example}

The reason for the tightness of the linear relaxation of
Example~\ref{eg:bipartitematching} is in the following property of the
matrix of constraints.

\begin{marginfigure}
  The criteria of total unimodularity given in Definition~\ref{def:tum} is
  not efficiently verifiable. Instead there are other sufficient (but
  maybe not necessary) criteria which can be verified in polynomial time.
\end{marginfigure}

\begin{definition}\label{def:tum}
  A matrix $ M $ is called \introduceterm{totally unimodular} if every
  square submatrix of $ M $ has determinant in $\{-1,0,1\}$.
\end{definition}

\begin{theorem}
  Consider the polyhedron $ P=\{x | Ax=b, x\geq 0\}$.  If $ A $ is
  totally unimodular and $ b $ is integer then the vertices of $ P$
  are all integers.
\end{theorem}

The previous theorem implies that an integer program with total
unimodular matrix of constrains can be solved efficiently by solving
its linear relaxation.


\section{Integrality gaps and Rounding}

In the case of Example~\ref{eg:bipartitematching}, we saw that the
integer program and its linear relaxation have the same optimum, but
this is not necessarily the same. Actually the gap between the integer
and the fractional optimum can be as large as possible.\footnote{%
  Consider the problem of computing the maximum independent set of a
  graph $ G=(V,E) $. In this case we want to maximize the objective
  function $ \sum_{v\in V} x_{v} $ under the assumption that $
  x_{u}+x_{v}\leq 1 $ for every pair $ \{u,v\}\in E$ for variables $
  \varboolean{x_{v}}$. If we consider the \stressterm{complete graph}
  then the maximum independent set has size at most 1, but the linear
  relaxation has a solution of value $ |V|/2 $, which is the vector $
  (\frac{1}{2},\frac{1}{2},\ldots,\frac{1}{2})$.}

For an optimization problem we may consider $\sOPT$, the integer
optimum, and $ \sFRAC$, which is the optimum of the relaxation. A
strategy to solve integer program is to relax it in some way, and to
\introduceterm{round its fractional solution} in order to get an
integer feasible solution. The rounding techniques is strongly
dependent on the particular optimization problem and it is out of the
scope of this lecture.
%
Let $ \sROUND $ denotes such solution, then for (say) a minimization problem
we have
\begin{equation}
  \label{eq:round_opt_frac}
  \sFRAC \leq \sOPT \leq \sROUND
\end{equation}

\begin{definition}
  Given an integer program and its relaxation, the ratio 
\begin{equation}
  \frac{\sOPT}{\sFRAC}
\end{equation}
is the \introduceterm{integrality gap} of the relaxation.
\end{definition}

Very often the analysis of an approximation algorithm is achieved by
determining the ratio between the fractional optimum and its rounded
value, $ \frac{\sROUND}{\sFRAC} $. The latter is bounded from below by
the integrality gap, 

\section{Improving the linear relaxations}

We discussed that the gap between the relaxed program and the integer
one is a key concept correlated with approximation algorithms and in
particular with their analysis. In order to get better relaxation we
can add \stressterm{additional inequalities} to the initial
program. Such inequalities are valid for integer solutions but still
shave off some of the fractional solutions.

Given an arbitrary linear program let $ \sol{P}$ be the convex hull of
its feasible solutions, and $ \sol{P}_{I} $ the convex hull of the
integer solutions. Of course $ \sol{P}_{I}\subseteq\sol{P} $. The
idea is to find a way to efficiently identify a convex set $ E $ such
that $ \sol{P_{I}}\subseteq \sol{E} \subseteq\sol{P}_{I} $. 
If the gap between the optimal solution in $ \sol{E} $
and the $ \sol{P}_{I} $ is strictly smaller than the integrality gap of the
original linear program, that would give a better handle to solve the
combinatorial problem. Even better if $ \sol{E}=\sol{P}_{I} $: then
the problem is solved exactly.

If $ \PTIME\neq\NP $ there is not general way to obtain $\sol{P}_{I} $
by adding only a polynomial number of linear inequalities.

\introduceterm{Cutting planes:} was introduced by
Gomory\cite{gomory1958outline}. His original way to cut fractional
solution is to notice that for an inequality $ a^{T}x \leq v $
where $ a $ is an integer vector, then 
\begin{equation}
  a^{T}x \leq \lfloor b \rfloor
\end{equation}
is valid over integer solutions (but may cut some fractional
ones). This method has been employed effectively in integer
programming packages.

\introduceterm{Extended formulation}: the idea is to add new variables
and write a linear program on them in such a way that the projection
of the extended polytope $\sol{P}_{E}$ on the original variables is
closer or even equal to $ \sol{P}_{I} $.

Such extended formulation is ``problem dependent'' and there are very
nice connections between the number of inequalities needed to capture
$ \sol{P}_{I} $ and communication complexity. Fiorini
et.al.\cite{fiorini2007linear} proved that there is no polynomial size
extended formulation for the canonical encoding of
\introduceterm{traveller salesman problem}.

In this course we deal with systematic ways to improve the relaxation
of the integer programs. In this lecture we are going to see some
techniques.

\section{\Lovasz-\Schrijver\ hierarchy}

Let us just focus on feasibility problems (no function to be
optimized). Then the point is to get close to a representation of $
\sol{P}_{I} $ in order to determine whether it is empty or not. We now
that if we could efficiently decide feasibility under
\introduceterm{quadratic inequalities} we could easily solve integer
program over boolean variables, since $ x_{i}\in\{0,1\} $ is equivalent
to $ x^{2}_{i} - x_{i} =0 $. 

The \Lovasz-\Schrijver\cite{lovasz1991cones} linear relaxation is
based on the idea of using quadratic inequalities to determine new
\stressterm{linear inequalities} which are valid for the integer
solutions but not for the fractional ones. Once the new linear
inequalities are in place, the quadratic inequalities are forgotten.
The resulting linear program is tighter than the initial one and we
can optimized against it.

The simplest way to describe \Lovasz-\Schrijver integer programming
relaxation is to interpret it as a proof system. Start with its linear
relaxation $ Ax \geq b, 0 \leq x \leq 1$, and call $ \sol{P} $ the set
of solutions of such inequalities\footnote{We focus on integer
  programs with boolean solutions here, thus the presence of
  inequalities $ 0 \leq x \leq 1 $ in the relaxation by
  default. Thus we can always assume we deal with polytopes here.}.

Every inequality $ \sum_{j}a_{i,j}x_{j} - b_{i} \leq 0$ is an axiom of
the proof system, together with axioms $ - x_{i} \leq 0$, $ x_{i} - 1
\leq 0$ and $ x_{i} - x^{2}_{i} \leq 0$.  The system allows to multiply a
linear inequality by a variable or by the complement of a variable
\begin{equation}\label{eq:lsmultiplication}
\AxiomC{$\sum_{j}c_{j} x_{j} - d \leq 0$}
\UnaryInfC{$ \sum_{j}c_{j}(x_{j}x_{i}) - d x_{i} \leq 0$}
\DisplayProof
\qquad
\AxiomC{$ \sum_{j}c_{j} x_{j} - d \leq 0$}
\UnaryInfC{$ \sum_{j}c_{j}x_{j}(1-x_{i}) - d (1-x_{i}) \leq 0$}
\DisplayProof,
\end{equation}

and to infer positive combinations of known inequalities

\begin{figure*}
\begin{equation}
\RightLabel{\ assuming $\alpha, \beta \geq 0$.}
\AxiomC{$c+\sum_{j}c_{j} x_{j} + \sum_{\{i,j\}} c_{\{i,j\}}x_{i}x_{j}\leq 0$}
\AxiomC{$d+\sum_{j}d_{j} x_{j} + \sum_{\{i,j\}} d_{\{i,j\}}x_{i}x_{j}\leq 0$}
\BinaryInfC{%
$\alpha (c+\sum_{j}c_{j} x_{j} + \sum_{\{i,j\}} c_{\{i,j\}}x_{i}x_{j})+
\beta (d+\sum_{j}d_{j} x_{j} + \sum_{\{i,j\}} d_{\{i,j\}}x_{i}x_{j}) \leq 0$}
\DisplayProof
\end{equation}
\end{figure*}

Quadratic inequalities can be used to derive new linear inequalities,
which in turn can be multiplied again. We will keep track on how many
multiplication steps are needed to derive a particular
inequality. This is the \introduceterm{rank of an inequality} in
\Lovasz-\Schrijver.

\begin{definition}[Rank of \Lovasz-\Schrijver]
  Consider a derivation of an inequality in \Lovasz-\Schrijver. The
  \introduceterm{rank of derivation} of an axiom is $ 0 $; the rank of
  a multiplication step is one plus the rank of the premise; the rank
  of a positive sum is the maximum among the rank of the premises.
  The \introduceterm{rank of an inequality} is the smallest among the
  rank all possible derivations of that inequality. The
  \introduceterm{rank of a point} is the smallest among the rank of
  all linear inequalities that are falsified by that point.   
\end{definition}

The set of solution of all linear inequalities of rank $ 0 $ is
denoted as $ \sol{P}_{0} $, and is exactly equal to $ \sol{P} $. The
polytope characterized by the points compatible with all linear
inequalities of rank $ t $ is denoted as $ \sol{P}_{t} $. It is
important to stress that while derivations are allowed to use
quadratic inequalities, the polytopes are just defined by the linear
ones. It holds that

\begin{equation}
  \sol{P}=\sol{P_{0}} \supseteq \sol{P}_{1} \subseteq \cdots
  \supseteq \sol{P}_{n-1} \subseteq \sol{P}_{n}=\sol{P}_{I}.
\end{equation}

where the last equation is proved in\cite{lovasz1991cones}.

\begin{theorem}[\Lovasz-\Schrijver, 1991]
  The polyhedron $ \sol{P}_{n} $ is equal to the convex hull of
  the integer points  $ \sol{P}_{I} $
\end{theorem}

There is a geometric interpretation of the \Lovasz-\Schrijver proof system
(indeed that would be the original definition). Given the a polytope $
\sol{P} $ defined on points in $ \RR^{n} $. We consider matrices over
$ \RR^{(n+1)\times(n+1)} $ with row and column indices going from $ 0
$ to $ n $, which must satisfy the following inequalities: $ x_{i0} =
x_{0i} = x_{ii} $ for all $ i \in [n] $; $ x_{ij}=x_{ji} $ for
every $ i,j \in [n]$; if $ a_{i} $




\section{Sherali-Adams hierarchy}

\subsection{Rank lower bounds}


% ------------------------- EPILOGUE ------------------------------
%\bibliography{theoryofcomputing,soscourse}
\bibliographystyle{alpha}

\end{document} 


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
